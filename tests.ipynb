{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "def load_prep_data(dataset_name):\n",
    "    if dataset_name in ['Adult', 'Bank']:\n",
    "        dataset = Dataset(dataset_name)\n",
    "        X, y, feature_names, x = dataset.load_data()\n",
    "        X_old, y_old, X_new, y_new = dataset.split_old_new_data(X, y)\n",
    "        return X_old, y_old, X_new, y_new, feature_names, x\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old data shape: [[2.60000e+01 2.22248e+05 9.00000e+00 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [4.90000e+01 1.33729e+05 1.00000e+01 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [5.50000e+01 1.02058e+05 6.00000e+00 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " ...\n",
      " [4.40000e+01 2.48973e+05 1.30000e+01 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [2.20000e+01 1.48187e+05 7.00000e+00 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [3.50000e+01 3.10950e+04 1.00000e+01 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]], Positive samples: 3536\n",
      "New data shape: [[2.80000e+01 3.38409e+05 1.30000e+01 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [5.20000e+01 2.09642e+05 9.00000e+00 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [3.00000e+01 1.41297e+05 1.30000e+01 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " ...\n",
      " [2.70000e+01 2.57302e+05 1.20000e+01 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [4.00000e+01 1.54374e+05 9.00000e+00 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [5.20000e+01 2.87927e+05 9.00000e+00 ... 1.00000e+00 0.00000e+00\n",
      "  0.00000e+00]], Positive samples: 4305\n"
     ]
    }
   ],
   "source": [
    "X_old, y_old, X_new, y_new, feature_names, x = load_prep_data('Adult')\n",
    "# Verify the split\n",
    "print(f\"Old data shape: {X_old}, Positive samples: {y_old.sum()}\")\n",
    "print(f\"New data shape: {X_new}, Positive samples: {y_new.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Models.models import train_old_model, train_new_models\n",
    "from Models.lct import LocalCorrectionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n###############################################################################\\n# Test Function\\n###############################################################################\\ndef test_local_correction_tree():\\n    \"\"\"\\n    Tests LocalCorrectionTree using random data.\\n    Includes assertions for each main step: fit, predict, prune, simplify.\\n    \"\"\"\\n    from numpy.random import rand, randint, randn, seed\\n\\n    seed(42)\\n    n_samples = 60\\n    n_features = 5\\n    n_classes = 3\\n    X = rand(n_samples, n_features)\\n    y = randint(0, n_classes, size=n_samples)\\n    old_scores = randn(n_samples, n_classes)\\n\\n    # Instantiate LCT\\n    lct = LocalCorrectionTree(lambda_reg=0.1, max_depth=3, min_samples_leaf=5)\\n\\n    lct.fit(X, y, old_scores)\\n    corrections = lct.predict(X)\\n    assert corrections.shape == (n_samples, n_classes), \"Predict output shape is incorrect\"\\n\\n    pre_prune_nodes = len(lct.nodes)\\n    lct.prune(X, y, old_scores, nprune=2)\\n    post_prune_nodes = len(lct.nodes)\\n    assert post_prune_nodes <= pre_prune_nodes, \"Prune should not increase node count\"\\n\\n    pre_simplify_nodes = len(lct.nodes)\\n    lct.simplify()\\n    post_simplify_nodes = len(lct.nodes)\\n    assert post_simplify_nodes <= pre_simplify_nodes, \"Simplify should not increase node count\"\\n\\n    print(\"All tests for LocalCorrectionTree passed successfully!\")\\n\\n###############################################################################\\n# Example: Randomly Sample \"lambda_reg\" with Optuna\\n###############################################################################\\ndef objective(trial):\\n    \"\"\"\\n    Optuna objective function that samples lambda_reg in [0.05, 0.5],\\n    then trains an LCT on random data and returns a dummy \\'loss\\'.\\n    Replace this dummy training code with real data if you want a meaningful search.\\n    \"\"\"\\n    # Sample random lambda\\n    lambda_reg = trial.suggest_float(\"lambda_reg\", 0.05, 0.5, log=False)\\n\\n    np.random.seed(42)\\n    X = np.random.rand(50, 5)\\n    y = np.random.randint(0, 3, size=50)\\n    old_scores = np.random.randn(50, 3)\\n\\n    lct_temp = LocalCorrectionTree(\\n        lambda_reg=lambda_reg,\\n        max_depth=3,\\n        min_samples_leaf=5,\\n    )\\n    # \"Train\" on the random data\\n    lct_temp.fit(X, y, old_scores)\\n\\n    # Evaluate some dummy metric (for demonstration).\\n    # Typically, you\\'d do cross-validation or measure real performance here.\\n    preds = lct_temp.predict(X)\\n    # We\\'ll create a dummy \"loss\" = sum of squares of corrections, smaller is \"better\"\\n    dummy_loss = float(np.sum(preds**2))\\n    return dummy_loss\\n\\n#if __name__ == \"__main__\":\\n    # 1. Run your LCT tests\\n    #test_local_correction_tree()\\n\\n    # 2. Optimizing with real random sampling for lambda_reg\\n    #import optuna\\n    #study = optuna.create_study(direction=\"minimize\")\\n    #study.optimize(objective, n_trials=10)  # e.g. 10 random trials\\n\\n    # 3. Print best trial and use it to instantiate LCT\\n    #best_trial = study.best_trial\\n    #best_lambda = best_trial.params[\"lambda_reg\"]\\n    #print(f\"\\nOptuna found best lambda_reg={best_lambda:.3f} with objective={best_trial.value:.3f}.\")\\n\\n    ## 4. Example: build a final LCT with the best lambda and possibly real data\\n    #final_lct = LocalCorrectionTree(lambda_reg=best_lambda, max_depth=3, min_samples_leaf=5)\\n    #print(f\"Final LCT instantiated with lambda_reg={best_lambda:.3f}.\")'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "###############################################################################\n",
    "# Test Function\n",
    "###############################################################################\n",
    "def test_local_correction_tree():\n",
    "    \"\"\"\n",
    "    Tests LocalCorrectionTree using random data.\n",
    "    Includes assertions for each main step: fit, predict, prune, simplify.\n",
    "    \"\"\"\n",
    "    from numpy.random import rand, randint, randn, seed\n",
    "\n",
    "    seed(42)\n",
    "    n_samples = 60\n",
    "    n_features = 5\n",
    "    n_classes = 3\n",
    "    X = rand(n_samples, n_features)\n",
    "    y = randint(0, n_classes, size=n_samples)\n",
    "    old_scores = randn(n_samples, n_classes)\n",
    "\n",
    "    # Instantiate LCT\n",
    "    lct = LocalCorrectionTree(lambda_reg=0.1, max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "    lct.fit(X, y, old_scores)\n",
    "    corrections = lct.predict(X)\n",
    "    assert corrections.shape == (n_samples, n_classes), \"Predict output shape is incorrect\"\n",
    "\n",
    "    pre_prune_nodes = len(lct.nodes)\n",
    "    lct.prune(X, y, old_scores, nprune=2)\n",
    "    post_prune_nodes = len(lct.nodes)\n",
    "    assert post_prune_nodes <= pre_prune_nodes, \"Prune should not increase node count\"\n",
    "\n",
    "    pre_simplify_nodes = len(lct.nodes)\n",
    "    lct.simplify()\n",
    "    post_simplify_nodes = len(lct.nodes)\n",
    "    assert post_simplify_nodes <= pre_simplify_nodes, \"Simplify should not increase node count\"\n",
    "\n",
    "    print(\"All tests for LocalCorrectionTree passed successfully!\")\n",
    "\n",
    "###############################################################################\n",
    "# Example: Randomly Sample \"lambda_reg\" with Optuna\n",
    "###############################################################################\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function that samples lambda_reg in [0.05, 0.5],\n",
    "    then trains an LCT on random data and returns a dummy 'loss'.\n",
    "    Replace this dummy training code with real data if you want a meaningful search.\n",
    "    \"\"\"\n",
    "    # Sample random lambda\n",
    "    lambda_reg = trial.suggest_float(\"lambda_reg\", 0.05, 0.5, log=False)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(50, 5)\n",
    "    y = np.random.randint(0, 3, size=50)\n",
    "    old_scores = np.random.randn(50, 3)\n",
    "\n",
    "    lct_temp = LocalCorrectionTree(\n",
    "        lambda_reg=lambda_reg,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=5,\n",
    "    )\n",
    "    # \"Train\" on the random data\n",
    "    lct_temp.fit(X, y, old_scores)\n",
    "\n",
    "    # Evaluate some dummy metric (for demonstration).\n",
    "    # Typically, you'd do cross-validation or measure real performance here.\n",
    "    preds = lct_temp.predict(X)\n",
    "    # We'll create a dummy \"loss\" = sum of squares of corrections, smaller is \"better\"\n",
    "    dummy_loss = float(np.sum(preds**2))\n",
    "    return dummy_loss\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    # 1. Run your LCT tests\n",
    "    #test_local_correction_tree()\n",
    "\n",
    "    # 2. Optimizing with real random sampling for lambda_reg\n",
    "    #import optuna\n",
    "    #study = optuna.create_study(direction=\"minimize\")\n",
    "    #study.optimize(objective, n_trials=10)  # e.g. 10 random trials\n",
    "\n",
    "    # 3. Print best trial and use it to instantiate LCT\n",
    "    #best_trial = study.best_trial\n",
    "    #best_lambda = best_trial.params[\"lambda_reg\"]\n",
    "    #print(f\"\\nOptuna found best lambda_reg={best_lambda:.3f} with objective={best_trial.value:.3f}.\")\n",
    "\n",
    "    ## 4. Example: build a final LCT with the best lambda and possibly real data\n",
    "    #final_lct = LocalCorrectionTree(lambda_reg=best_lambda, max_depth=3, min_samples_leaf=5)\n",
    "    #print(f\"Final LCT instantiated with lambda_reg={best_lambda:.3f}.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\n\\ndataset = Dataset(\"Adult\")\\nX, y, feature_names, raw_data = dataset.load_data()\\nX, y = np.array(X), np.array(y)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Train a simple logistic regression model as the \"old model\"\\nold_model = LogisticRegression(random_state=42)\\nold_model.fit(X_train, y_train)\\n\\n# Get old model scores on the test set\\nold_scores = old_model.predict_proba(X_test)\\n\\n# Instantiate and fit the Local Correction Tree\\nlct = LocalCorrectionTree()\\nlct.fit(X_test, y_test, old_scores)\\n\\n# Get corrections from LCT\\ncorrections = lct.predict(X_test)\\n\\n# Apply corrections to old model scores\\ncorrected_scores = old_scores + corrections\\n\\n# Evaluate the performance\\nold_accuracy = np.mean(np.argmax(old_scores, axis=1) == y_test)\\nnew_accuracy = np.mean(np.argmax(corrected_scores, axis=1) == y_test)\\n\\nprint(f\"Old model accuracy: {old_accuracy:.4f}\")\\nprint(f\"Corrected model accuracy: {new_accuracy:.4f}\")\\n\\n# Analyze the corrections\\nnon_zero_corrections = np.sum(np.any(corrections != 0, axis=1))\\nprint(f\"Number of samples with non-zero corrections: {non_zero_corrections}\")\\nprint(f\"Percentage of samples corrected: {non_zero_corrections / len(X_test) * 100:.2f}%\")\\n\\n# Print a few example corrections along with labels before and after correction\\nprint(\"\\nExample corrections:\")\\nfor i in range(10):\\n    old_label = np.argmax(old_scores[i])\\n    corrected_label = np.argmax(corrected_scores[i])\\n    print(f\"Sample {i}:\")\\n    print(f\"  Old scores: {old_scores[i]}\")\\n    print(f\"  Correction: {corrections[i]}\")\\n    print(f\"  New scores: {corrected_scores[i]}\")\\n    print(f\"  True label: {y_test[i]}\")\\n    print(f\"  Predicted label before correction: {old_label}\")\\n    print(f\"  Predicted label after correction: {corrected_label}\")\\n    print()\\n\\n# Visualize the tree structure and include labels before and after correction\\nlct.simplify()\\nprint(\"\\nSimplified tree structure:\")\\ndef print_tree(node_id, depth=0):\\n    feature_idx, threshold, w_node = lct.nodes[node_id]\\n    left_id, right_id = lct.children[node_id]\\n    \\n    if feature_idx == -1:\\n        print(\"  \" * depth + f\"Leaf: correction = {w_node}\")\\n    else:\\n        feature_name = feature_names[feature_idx]\\n        print(\"  \" * depth + f\"Node: {feature_name} <= {threshold:.4f}\")\\n        print_tree(left_id, depth + 1)\\n        print_tree(right_id, depth + 1)\\n\\nprint_tree(0) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dataset = Dataset(\"Adult\")\n",
    "X, y, feature_names, raw_data = dataset.load_data()\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple logistic regression model as the \"old model\"\n",
    "old_model = LogisticRegression(random_state=42)\n",
    "old_model.fit(X_train, y_train)\n",
    "\n",
    "# Get old model scores on the test set\n",
    "old_scores = old_model.predict_proba(X_test)\n",
    "\n",
    "# Instantiate and fit the Local Correction Tree\n",
    "lct = LocalCorrectionTree()\n",
    "lct.fit(X_test, y_test, old_scores)\n",
    "\n",
    "# Get corrections from LCT\n",
    "corrections = lct.predict(X_test)\n",
    "\n",
    "# Apply corrections to old model scores\n",
    "corrected_scores = old_scores + corrections\n",
    "\n",
    "# Evaluate the performance\n",
    "old_accuracy = np.mean(np.argmax(old_scores, axis=1) == y_test)\n",
    "new_accuracy = np.mean(np.argmax(corrected_scores, axis=1) == y_test)\n",
    "\n",
    "print(f\"Old model accuracy: {old_accuracy:.4f}\")\n",
    "print(f\"Corrected model accuracy: {new_accuracy:.4f}\")\n",
    "\n",
    "# Analyze the corrections\n",
    "non_zero_corrections = np.sum(np.any(corrections != 0, axis=1))\n",
    "print(f\"Number of samples with non-zero corrections: {non_zero_corrections}\")\n",
    "print(f\"Percentage of samples corrected: {non_zero_corrections / len(X_test) * 100:.2f}%\")\n",
    "\n",
    "# Print a few example corrections along with labels before and after correction\n",
    "print(\"\\nExample corrections:\")\n",
    "for i in range(10):\n",
    "    old_label = np.argmax(old_scores[i])\n",
    "    corrected_label = np.argmax(corrected_scores[i])\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  Old scores: {old_scores[i]}\")\n",
    "    print(f\"  Correction: {corrections[i]}\")\n",
    "    print(f\"  New scores: {corrected_scores[i]}\")\n",
    "    print(f\"  True label: {y_test[i]}\")\n",
    "    print(f\"  Predicted label before correction: {old_label}\")\n",
    "    print(f\"  Predicted label after correction: {corrected_label}\")\n",
    "    print()\n",
    "\n",
    "# Visualize the tree structure and include labels before and after correction\n",
    "lct.simplify()\n",
    "print(\"\\nSimplified tree structure:\")\n",
    "def print_tree(node_id, depth=0):\n",
    "    feature_idx, threshold, w_node = lct.nodes[node_id]\n",
    "    left_id, right_id = lct.children[node_id]\n",
    "    \n",
    "    if feature_idx == -1:\n",
    "        print(\"  \" * depth + f\"Leaf: correction = {w_node}\")\n",
    "    else:\n",
    "        feature_name = feature_names[feature_idx]\n",
    "        print(\"  \" * depth + f\"Node: {feature_name} <= {threshold:.4f}\")\n",
    "        print_tree(left_id, depth + 1)\n",
    "        print_tree(right_id, depth + 1)\n",
    "\n",
    "print_tree(0) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def _find_w(self, indices, old_scores, y, max_iter=10):\\n        #n_samples = len(indices)\\n        n_classes = old_scores.shape[1]\\n        \\n        w = np.zeros((1, n_classes))\\n        g_avg = np.zeros((1, n_classes))\\n        beta_t = 1.0  # Example sequence, adjust as needed\\n        \\n        for t in range(1, max_iter + 1):\\n            for i in indices:\\n                # Compute subgradient of the loss function for sample i\\n                sub_scores = old_scores[i] + w\\n                probs = np.exp(sub_scores) / np.sum(np.exp(sub_scores))\\n                subgradient = -np.eye(n_classes)[y[i]] + probs\\n                \\n                # Update average subgradient\\n                g_avg = (t - 1) / t * g_avg + 1 / t * subgradient\\n                \\n                # Update w using RDA formula\\n                w = w - beta_t / t * g_avg - self.lambda_reg * w\\n        \\n        return w'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def _find_w(self, indices, old_scores, y, max_iter=10):\n",
    "        #n_samples = len(indices)\n",
    "        n_classes = old_scores.shape[1]\n",
    "        \n",
    "        w = np.zeros((1, n_classes))\n",
    "        g_avg = np.zeros((1, n_classes))\n",
    "        beta_t = 1.0  # Example sequence, adjust as needed\n",
    "        \n",
    "        for t in range(1, max_iter + 1):\n",
    "            for i in indices:\n",
    "                # Compute subgradient of the loss function for sample i\n",
    "                sub_scores = old_scores[i] + w\n",
    "                probs = np.exp(sub_scores) / np.sum(np.exp(sub_scores))\n",
    "                subgradient = -np.eye(n_classes)[y[i]] + probs\n",
    "                \n",
    "                # Update average subgradient\n",
    "                g_avg = (t - 1) / t * g_avg + 1 / t * subgradient\n",
    "                \n",
    "                # Update w using RDA formula\n",
    "                w = w - beta_t / t * g_avg - self.lambda_reg * w\n",
    "        \n",
    "        return w'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_old_model(dataset, X, y, old_model):\n",
    "    \"\"\"\n",
    "    Evaluate the old model on the given dataset (X, y).\n",
    "    Returns a dictionary of metrics with lists of values.\n",
    "    \"\"\"\n",
    "    y_pred = old_model.predict(X)\n",
    "    is_multiclass = dataset in ['CTG', '7-point']\n",
    "    \n",
    "    if is_multiclass:\n",
    "        precision = precision_score(y, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y, y_pred, average='macro', zero_division=0)\n",
    "    else:\n",
    "        precision = precision_score(y, y_pred, zero_division=0)\n",
    "        recall = recall_score(y, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y, y_pred, zero_division=0)\n",
    "    \n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1': [f1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_new_models(dataset, models, X_test, y_test, base_models_dict):\n",
    "    \"\"\"\n",
    "    Evaluate new models including LCT on the test set.\n",
    "    Returns metrics formatted for error bar plotting.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'Accuracy': {},\n",
    "        'Precision': {},\n",
    "        'Recall': {},\n",
    "        'F1': {}\n",
    "    }\n",
    "    \n",
    "    is_multiclass = dataset in ['CTG', '7-point']\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        base_model_name = model_name.replace('+', '').replace('C', '')\n",
    "        old_scores_test = base_models_dict[base_model_name].predict_proba(X_test)\n",
    "        \n",
    "        if model_name == 'Ours':  # LCT model\n",
    "            corrections = model.predict(X_test)\n",
    "            corrected_scores = old_scores_test + corrections\n",
    "            y_pred = np.argmax(corrected_scores, axis=1)\n",
    "        elif model_name.endswith('+') or model_name.endswith('+C'):\n",
    "            # Models with old scores as additional features\n",
    "            X_test_with_scores = np.hstack([X_test, old_scores_test])\n",
    "            y_pred = model.predict(X_test_with_scores)\n",
    "        else:\n",
    "            # Base models\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if is_multiclass:\n",
    "            precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        metrics['Accuracy'][model_name] = accuracy\n",
    "        metrics['Precision'][model_name] = precision\n",
    "        metrics['Recall'][model_name] = recall\n",
    "        metrics['F1'][model_name] = f1\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(dataset, metrics, model_names):\n",
    "    \"\"\"\n",
    "    Create comparison bar plots for model metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    \n",
    "    # Define model colors and markers\n",
    "    colors = {\n",
    "        'L1-LR': '#1f77b4',\n",
    "        'L1-LR+': '#1f77b4',\n",
    "        'L2-LR': '#ff7f0e',\n",
    "        'L2-LR+': '#ff7f0e',\n",
    "        'DT': '#2ca02c',\n",
    "        'DT+': '#2ca02c',\n",
    "        'RF': '#8c564b',\n",
    "        'RF+': '#8c564b',\n",
    "        'LGBM': '#9467bd',\n",
    "        'LGBM+': '#9467bd',\n",
    "        'LGBM+C': '#9467bd',\n",
    "        'Ours': '#d62728',\n",
    "        'Old': '#7f7f7f'\n",
    "    }\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_names):\n",
    "        ax = axes[idx]\n",
    "        y_pos = np.arange(len(model_names))\n",
    "        values = metrics[metric]\n",
    "        \n",
    "        # Plot bar plots\n",
    "        for i, (name, value) in enumerate(zip(model_names, values)):\n",
    "            color = colors.get(name)\n",
    "            mean_val = np.mean(value)\n",
    "            std_val = np.std(value)\n",
    "            \n",
    "            # Plot bar\n",
    "            ax.bar(i, mean_val, color=color, alpha=0.7, width=0.8)\n",
    "            \n",
    "            # Plot error bars\n",
    "            ax.errorbar(i, mean_val, yerr=std_val, fmt='none', capsize=3, color='black', elinewidth=1, capthick=1)\n",
    "        \n",
    "        ax.set_yticks(np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 5))\n",
    "        ax.set_xticks(y_pos)\n",
    "        ax.set_xticklabels(model_names, rotation=90)\n",
    "        ax.set_title(f'({chr(97+idx)}) {metric} ({dataset})')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Set y-axis limits\n",
    "        min_val = min([np.mean(v) for v in values]) - 0.05\n",
    "        max_val = max([np.mean(v) for v in values]) + 0.05\n",
    "        ax.set_ylim(min_val, max_val)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_metrics(metrics, models, dataset, color_scheme):\n",
    "    \"\"\"Create subplots for all metrics with different colors for each model\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "            \n",
    "    for idx, (ax, metric) in enumerate(zip(axes, metric_names)):\n",
    "        means = []\n",
    "        stds = []\n",
    "                \n",
    "        for model in models:\n",
    "            mean_val = np.mean(metrics[metric][model])\n",
    "            std_val = np.std(metrics[metric][model])\n",
    "            means.append(mean_val)\n",
    "            stds.append(std_val)\n",
    "                    \n",
    "            # Create horizontal error bar plot with different colors\n",
    "            y_pos = np.arange(len(models))\n",
    "            ax.errorbar(mean_val, models.index(model), xerr=std_val, \n",
    "                        fmt='o', capsize=5, color=color_scheme[model],\n",
    "                        label=model, markersize=8, elinewidth=2, capthick=2)\n",
    "                \n",
    "            # Customize subplot\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(models)\n",
    "            ax.set_xlabel(metric)\n",
    "            ax.set_title(f'({chr(97+idx)}) {metric} ({dataset})')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "                \n",
    "            # Set x-axis limits with margin\n",
    "            margin = 0.02\n",
    "            ax.set_xlim(min(means) - margin, max(means) + margin)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(lct, node_id=0, depth=0, feature_names=None):\n",
    "    \"\"\"\n",
    "    Print the Local Correction Tree structure.\n",
    "    Args:\n",
    "        lct: LocalCorrectionTree instance\n",
    "        node_id: Current node ID\n",
    "        depth: Current depth in tree\n",
    "        feature_names: Optional list of feature names\n",
    "    \"\"\"\n",
    "    feature_idx, threshold, w_node = lct.nodes[node_id]\n",
    "    left_id, right_id = lct.children[node_id]\n",
    "    \n",
    "    indent = \"  \" * depth\n",
    "    if feature_idx == -1:\n",
    "        print(f\"{indent}Leaf: correction = {w_node.round(4)}\")\n",
    "    else:\n",
    "        feature = f\"X[{feature_idx}]\" if feature_names is None else feature_names[feature_idx]\n",
    "        print(f\"{indent}Node: {feature} <= {threshold:.4f}\")\n",
    "        print_tree(lct, left_id, depth + 1, feature_names)\n",
    "        print_tree(lct, right_id, depth + 1, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets = [\\'Adult\\', \\'Bank\\']\\nfor dataset in datasets:\\n    print(f\"\\nProcessing {dataset} dataset\")\\n    \\n    # Define only base models\\n    base_models = [\\'L1-LR\\', \\'L2-LR\\', \\'DT\\', \\'RF\\', \\'LGBM\\']\\n    \\n    # 1. Load data\\n    if dataset == \\'7-point\\':\\n        X_old, y_old, X_new, y_new, X_clinical = load_prep_data(dataset)\\n    else:\\n        X_old, y_old, X_new, y_new, feature_names, x = load_prep_data(dataset)\\n    \\n    X_new, X_old = np.array(X_new), np.array(X_old)\\n    y_new, y_old = np.array(y_new), np.array(y_old)\\n    \\n    # 2. Train base models on old data\\n    base_models_dict = train_old_model(dataset, X_new, X_old, y_new, y_old)\\n    \\n    # 3. Perform 5-fold cross validation\\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\\n    all_metrics = {\\n        \\'Accuracy\\': {model: [] for model in base_models},\\n        \\'Precision\\': {model: [] for model in base_models},\\n        \\'Recall\\': {model: [] for model in base_models},\\n        \\'F1\\': {model: [] for model in base_models}\\n    }\\n    \\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X_old, y_old)):\\n        print(f\"Processing fold {fold + 1}/5\")\\n        \\n        X_train, X_test = X_old[train_idx], X_old[test_idx]\\n        y_train, y_test = y_old[train_idx], y_old[test_idx]\\n        \\n        # Evaluate each base model individually\\n        for model_name in base_models:\\n            fold_metrics = evaluate_old_model(dataset, X_test, y_test, base_models_dict[model_name])\\n            \\n            for metric in all_metrics:\\n                all_metrics[metric][model_name].extend(fold_metrics[metric])\\n    \\n    # 4. Prepare metrics for plotting\\n    plot_metrics = {\\n        \\'Accuracy\\': [],\\n        \\'Precision\\': [],\\n        \\'Recall\\': [],\\n        \\'F1\\': []\\n    }\\n    \\n    for metric in plot_metrics:\\n        for model in base_models:\\n            plot_metrics[metric].append(all_metrics[metric][model])\\n    \\n    # 5. Create and save plot\\n    fig = plot_metrics_comparison(dataset, plot_metrics, base_models)\\n    plt.savefig(f\\'{dataset}_base_models_comparison.png\\', bbox_inches=\\'tight\\', dpi=400)\\n    plt.close()\\n    \\n    # 6. Print numerical results\\n    print(f\"\\nBase Model Results for {dataset}:\")\\n    for metric in plot_metrics:\\n        print(f\"\\n{metric}:\")\\n        for name, values in zip(base_models, plot_metrics[metric]):\\n            mean_val = np.mean(values)\\n            std_val = np.std(values)\\n            print(f\"{name}: {mean_val:.4f} ± {std_val:.4f}\")\\n\\nprint(\"\\nDone evaluating base models.\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''datasets = ['Adult', 'Bank']\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nProcessing {dataset} dataset\")\n",
    "    \n",
    "    # Define only base models\n",
    "    base_models = ['L1-LR', 'L2-LR', 'DT', 'RF', 'LGBM']\n",
    "    \n",
    "    # 1. Load data\n",
    "    if dataset == '7-point':\n",
    "        X_old, y_old, X_new, y_new, X_clinical = load_prep_data(dataset)\n",
    "    else:\n",
    "        X_old, y_old, X_new, y_new, feature_names, x = load_prep_data(dataset)\n",
    "    \n",
    "    X_new, X_old = np.array(X_new), np.array(X_old)\n",
    "    y_new, y_old = np.array(y_new), np.array(y_old)\n",
    "    \n",
    "    # 2. Train base models on old data\n",
    "    base_models_dict = train_old_model(dataset, X_new, X_old, y_new, y_old)\n",
    "    \n",
    "    # 3. Perform 5-fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    all_metrics = {\n",
    "        'Accuracy': {model: [] for model in base_models},\n",
    "        'Precision': {model: [] for model in base_models},\n",
    "        'Recall': {model: [] for model in base_models},\n",
    "        'F1': {model: [] for model in base_models}\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_old, y_old)):\n",
    "        print(f\"Processing fold {fold + 1}/5\")\n",
    "        \n",
    "        X_train, X_test = X_old[train_idx], X_old[test_idx]\n",
    "        y_train, y_test = y_old[train_idx], y_old[test_idx]\n",
    "        \n",
    "        # Evaluate each base model individually\n",
    "        for model_name in base_models:\n",
    "            fold_metrics = evaluate_old_model(dataset, X_test, y_test, base_models_dict[model_name])\n",
    "            \n",
    "            for metric in all_metrics:\n",
    "                all_metrics[metric][model_name].extend(fold_metrics[metric])\n",
    "    \n",
    "    # 4. Prepare metrics for plotting\n",
    "    plot_metrics = {\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': []\n",
    "    }\n",
    "    \n",
    "    for metric in plot_metrics:\n",
    "        for model in base_models:\n",
    "            plot_metrics[metric].append(all_metrics[metric][model])\n",
    "    \n",
    "    # 5. Create and save plot\n",
    "    fig = plot_metrics_comparison(dataset, plot_metrics, base_models)\n",
    "    plt.savefig(f'{dataset}_base_models_comparison.png', bbox_inches='tight', dpi=400)\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Print numerical results\n",
    "    print(f\"\\nBase Model Results for {dataset}:\")\n",
    "    for metric in plot_metrics:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for name, values in zip(base_models, plot_metrics[metric]):\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            print(f\"{name}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "print(\"\\nDone evaluating base models.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Adult dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-02-04 19:35:20,251] A new study created in memory with name: no-name-713bd4fe-dd6c-4f3a-bf42-97d09ee4e3c9\n",
      "[I 2025-02-04 19:35:20,278] Trial 0 finished with value: 0.11767086558405422 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.035758115847772456, 'max_features': 0.8792230059669739, 'min_impurity_decrease': 0.5161513410274526}. Best is trial 0 with value: 0.11767086558405422.\n",
      "[I 2025-02-04 19:35:20,344] Trial 1 finished with value: 0.6590225255303823 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00020436669124228483, 'max_features': 0.7927606462544301, 'min_impurity_decrease': 1.0053437016871115e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,419] Trial 2 finished with value: 0.6475658472527505 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0113140569232298, 'max_features': 0.834043890233088, 'min_impurity_decrease': 3.5751604592824435e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,452] Trial 3 finished with value: 0.6036660893139995 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.014138565384821042, 'max_features': 0.35285497838970836, 'min_impurity_decrease': 1.1358431700246525e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,483] Trial 4 finished with value: 0.5485886677084962 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.08464013509954035, 'max_features': 0.37218995710838837, 'min_impurity_decrease': 8.410755837652352e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,504] Trial 5 finished with value: 0.11767086558405422 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0024412160237533443, 'max_features': 0.5282603334213468, 'min_impurity_decrease': 0.26238432004477896}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,539] Trial 6 finished with value: 0.5782373564270781 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.007039263622095377, 'max_features': 0.7386372699298733, 'min_impurity_decrease': 0.05997813756166841}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,596] Trial 7 finished with value: 0.6509301744830652 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00012676848490724712, 'max_features': 0.45371785325542546, 'min_impurity_decrease': 5.969304642899925e-05}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,649] Trial 8 finished with value: 0.6272014875130466 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.029545314802185196, 'max_features': 0.6354329834735712, 'min_impurity_decrease': 3.6627516499286026e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,673] Trial 9 finished with value: 0.6093081313187899 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.002200039504796845, 'max_features': 0.11431073223304743, 'min_impurity_decrease': 1.134149220373831e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,747] Trial 10 finished with value: 0.6483056018931412 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00010802732277071657, 'max_features': 0.9868674568169988, 'min_impurity_decrease': 0.0025661299266143385}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,808] Trial 11 finished with value: 0.6487474193669683 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.00013703329326178983, 'max_features': 0.5180327448950589, 'min_impurity_decrease': 0.00015496666778299135}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,857] Trial 12 finished with value: 0.6423174470617788 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.00046652841376014216, 'max_features': 0.3724217227323249, 'min_impurity_decrease': 3.5675297398016356e-05}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,920] Trial 13 finished with value: 0.6580039742322211 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0003828495490183649, 'max_features': 0.6718521298016255, 'min_impurity_decrease': 1.15169536878027e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:20,983] Trial 14 finished with value: 0.6487186386348994 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0005999409301533299, 'max_features': 0.7093835011619237, 'min_impurity_decrease': 1.1289001281983351e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,044] Trial 15 finished with value: 0.6545015279772759 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0004965248205933234, 'max_features': 0.6527756613245176, 'min_impurity_decrease': 1.0007777154720756e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,113] Trial 16 finished with value: 0.6582860650320229 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0009531247314022253, 'max_features': 0.8259136093725319, 'min_impurity_decrease': 5.336200526220432e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,185] Trial 17 finished with value: 0.6515945501926171 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0009802327002028345, 'max_features': 0.9976001859077399, 'min_impurity_decrease': 8.405140635519196e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,297] Trial 18 finished with value: 0.6518212130151567 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.0013175921579693891, 'max_features': 0.8705236434548892, 'min_impurity_decrease': 9.417211130222722e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,366] Trial 19 finished with value: 0.6553801682128163 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0002512133778929651, 'max_features': 0.8202115968845684, 'min_impurity_decrease': 0.0009996135287369424}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,428] Trial 20 finished with value: 0.6435631074837619 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.005333422778376631, 'max_features': 0.7731698029962202, 'min_impurity_decrease': 9.37182512120954e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,485] Trial 21 finished with value: 0.6501501305634845 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0004144803676739108, 'max_features': 0.638483584279118, 'min_impurity_decrease': 2.469920893058694e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,571] Trial 22 finished with value: 0.6555101976439973 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00023314850934184862, 'max_features': 0.9264824090801931, 'min_impurity_decrease': 2.3248450917087058e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,636] Trial 23 finished with value: 0.6547352861470427 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.001137170703480179, 'max_features': 0.7163461513815886, 'min_impurity_decrease': 4.407312603119383e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,713] Trial 24 finished with value: 0.6566783693137983 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00023355126446338614, 'max_features': 0.7846653302376319, 'min_impurity_decrease': 4.764264318673604e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,767] Trial 25 finished with value: 0.6441291540929123 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0007944572508712089, 'max_features': 0.6021618921923622, 'min_impurity_decrease': 6.222164840277551e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,844] Trial 26 finished with value: 0.6555478446297204 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0003035145617196926, 'max_features': 0.9153982127744987, 'min_impurity_decrease': 1.030818627520871e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,919] Trial 27 finished with value: 0.6464241097018486 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.0015509830014819935, 'max_features': 0.6993249854345905, 'min_impurity_decrease': 5.284777491736791e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:21,978] Trial 28 finished with value: 0.6354607656506871 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.004116229803317471, 'max_features': 0.5744369056321135, 'min_impurity_decrease': 1.734251156006087e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,048] Trial 29 finished with value: 0.6537147448589764 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.00015821040186503717, 'max_features': 0.8890479352594041, 'min_impurity_decrease': 1.651234760034085e-05}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,130] Trial 30 finished with value: 0.6495288428321396 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.0006290546223940559, 'max_features': 0.8072467425521761, 'min_impurity_decrease': 3.4356525346198483e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,209] Trial 31 finished with value: 0.6541248646766187 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00020621642444004727, 'max_features': 0.773936073767701, 'min_impurity_decrease': 8.98247805884094e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,283] Trial 32 finished with value: 0.6562377903526353 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00034862537883473456, 'max_features': 0.842430306630047, 'min_impurity_decrease': 3.33191427007466e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,360] Trial 33 finished with value: 0.6538028303116553 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00018649412636010878, 'max_features': 0.7697336448767236, 'min_impurity_decrease': 2.7357356090356392e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,435] Trial 34 finished with value: 0.654005909144891 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0001001820687150949, 'max_features': 0.9458392612104709, 'min_impurity_decrease': 1.5356545523404806e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,497] Trial 35 finished with value: 0.6511794833839435 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0007786696874019538, 'max_features': 0.6708849318373975, 'min_impurity_decrease': 3.1719011702068487e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,590] Trial 36 finished with value: 0.6508699410498169 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.0003143527662342029, 'max_features': 0.8427097506637745, 'min_impurity_decrease': 0.00016521891234630998}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,620] Trial 37 finished with value: 0.6252929701049691 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.0022537044374365367, 'max_features': 0.13034917719767697, 'min_impurity_decrease': 2.703475306150157e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,682] Trial 38 finished with value: 0.6429705481016712 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0016465026811401573, 'max_features': 0.4745383845842284, 'min_impurity_decrease': 7.493042005824371e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,731] Trial 39 finished with value: 0.5847897680909558 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.09834182967631003, 'max_features': 0.7559418623356582, 'min_impurity_decrease': 0.0010096718713927578}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,769] Trial 40 finished with value: 0.5848053169324935 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.009970471726218932, 'max_features': 0.5906060540292952, 'min_impurity_decrease': 0.02727549527513715}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,846] Trial 41 finished with value: 0.6553426058821796 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00035765157100782567, 'max_features': 0.8402252415523177, 'min_impurity_decrease': 2.759328543789508e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:22,932] Trial 42 finished with value: 0.6522148852575229 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00028106744574470696, 'max_features': 0.8720800374827222, 'min_impurity_decrease': 3.1894604743814414e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,007] Trial 43 finished with value: 0.6536402275917156 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00017378513903585003, 'max_features': 0.8042141287739683, 'min_impurity_decrease': 6.339424817113013e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,064] Trial 44 finished with value: 0.6164767356256912 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.025133069805107548, 'max_features': 0.6754397536011248, 'min_impurity_decrease': 1.3824098387100506e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,128] Trial 45 finished with value: 0.65583935347073 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.000543311024342713, 'max_features': 0.7417351854647054, 'min_impurity_decrease': 3.273622152875235e-05}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,160] Trial 46 finished with value: 0.11767086558405422 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00040566888877852986, 'max_features': 0.8045382994143006, 'min_impurity_decrease': 0.8246419213043469}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,245] Trial 47 finished with value: 0.6566280127034516 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00014606884522297192, 'max_features': 0.9651317766443827, 'min_impurity_decrease': 5.989764838253496e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,366] Trial 48 finished with value: 0.6549290860016589 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.0001425867619493388, 'max_features': 0.9603162186467351, 'min_impurity_decrease': 2.0252129445949085e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,514] Trial 49 finished with value: 0.653516231117362 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00012101951076078829, 'max_features': 0.8912583582196442, 'min_impurity_decrease': 5.805020619037604e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,547] Trial 50 finished with value: 0.5778967754455395 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00021729321117033946, 'max_features': 0.19337622758388917, 'min_impurity_decrease': 1.7151891769202902e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,624] Trial 51 finished with value: 0.657266233043171 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0007385252369435865, 'max_features': 0.8461030584665753, 'min_impurity_decrease': 5.487401255714073e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,706] Trial 52 finished with value: 0.6551737021776812 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0007886193769600996, 'max_features': 0.9877701888232445, 'min_impurity_decrease': 1.0815497266245889e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,787] Trial 53 finished with value: 0.6566814403954927 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0005757407850297146, 'max_features': 0.9074906658332337, 'min_impurity_decrease': 7.667919503195471e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,860] Trial 54 finished with value: 0.6414101094375882 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0006277605185186459, 'max_features': 0.9076496470003386, 'min_impurity_decrease': 6.220209457356998e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:23,929] Trial 55 finished with value: 0.6502079458635527 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0009171478323184796, 'max_features': 0.7258658773707225, 'min_impurity_decrease': 5.35148658181322e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,015] Trial 56 finished with value: 0.6520239751611822 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.0014396621203918837, 'max_features': 0.8717138064580225, 'min_impurity_decrease': 2.0857976468802157e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,085] Trial 57 finished with value: 0.6469132541454158 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0029607445795060607, 'max_features': 0.7875672427823471, 'min_impurity_decrease': 1.0186925934628689e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,146] Trial 58 finished with value: 0.6559040957116096 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0004889114286989309, 'max_features': 0.6969176780369726, 'min_impurity_decrease': 1.1176760950449051e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,246] Trial 59 finished with value: 0.6450243531256369 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0010543429670957416, 'max_features': 0.8452230939424535, 'min_impurity_decrease': 1.816374031227641e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,343] Trial 60 finished with value: 0.6483619927519978 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002565950629094346, 'max_features': 0.7419946381426233, 'min_impurity_decrease': 8.908568011812511e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,428] Trial 61 finished with value: 0.656477453162612 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0004325873860959897, 'max_features': 0.9517476294740624, 'min_impurity_decrease': 4.611015995550975e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,506] Trial 62 finished with value: 0.6505548906290622 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0001478909432292011, 'max_features': 0.9191464074971991, 'min_impurity_decrease': 3.0995763677243783e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,578] Trial 63 finished with value: 0.658132220263321 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00023451186788517607, 'max_features': 0.8634133278078974, 'min_impurity_decrease': 5.929893716014488e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,638] Trial 64 finished with value: 0.6551299487914809 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0006818146034413086, 'max_features': 0.6147119127347717, 'min_impurity_decrease': 1.438076148543287e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,702] Trial 65 finished with value: 0.6540585531140959 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.00023177940516874373, 'max_features': 0.8150779255866175, 'min_impurity_decrease': 1.8607188559350891e-06}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,783] Trial 66 finished with value: 0.6506560154545326 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.001841504290599479, 'max_features': 0.859043484851115, 'min_impurity_decrease': 3.522831324477908e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,837] Trial 67 finished with value: 0.5786591022274287 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.05707496380937807, 'max_features': 0.7860647992194919, 'min_impurity_decrease': 8.695437028902416e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,893] Trial 68 finished with value: 0.6537592720673427 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00033843938212119705, 'max_features': 0.5474655445475799, 'min_impurity_decrease': 1.9347294160911857e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:24,976] Trial 69 finished with value: 0.6503102946556232 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.0012343552762946613, 'max_features': 0.8969244143999794, 'min_impurity_decrease': 3.747901989940056e-07}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:25,046] Trial 70 finished with value: 0.6525111724258513 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0005458867047950541, 'max_features': 0.934192610027328, 'min_impurity_decrease': 0.00012258554197531064}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:25,140] Trial 71 finished with value: 0.6554564485138773 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00019000237549929845, 'max_features': 0.9680133957317645, 'min_impurity_decrease': 5.2327718785933966e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:25,248] Trial 72 finished with value: 0.658098935508038 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00011343891966500923, 'max_features': 0.8245689469212422, 'min_impurity_decrease': 6.495772832770879e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:25,323] Trial 73 finished with value: 0.6573882620773248 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00012635114534916673, 'max_features': 0.8235173599760086, 'min_impurity_decrease': 1.6375679301448637e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:25,395] Trial 74 finished with value: 0.6526754946395351 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00011067986237993755, 'max_features': 0.8223667871381891, 'min_impurity_decrease': 1.499136343086927e-08}. Best is trial 1 with value: 0.6590225255303823.\n",
      "[I 2025-02-04 19:35:25,471] Trial 75 finished with value: 0.6597011616896998 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00012573076055523465, 'max_features': 0.8616746484264018, 'min_impurity_decrease': 3.995081050921761e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:25,542] Trial 76 finished with value: 0.6583467306242777 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00011691434938503414, 'max_features': 0.7622219737545252, 'min_impurity_decrease': 3.8483517196700516e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:25,612] Trial 77 finished with value: 0.653722597413709 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0001194053791305103, 'max_features': 0.7598829936179247, 'min_impurity_decrease': 2.8493579657180695e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:25,671] Trial 78 finished with value: 0.6421490252550868 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.00010065225589778042, 'max_features': 0.6862609519410022, 'min_impurity_decrease': 1.0234339945997633e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:25,733] Trial 79 finished with value: 0.6511790921407893 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00013236531423856254, 'max_features': 0.6565179405088003, 'min_impurity_decrease': 2.983678616286853e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:25,798] Trial 80 finished with value: 0.6556004147625022 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00017272292388128226, 'max_features': 0.7229514419184906, 'min_impurity_decrease': 7.864846594214192e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:25,873] Trial 81 finished with value: 0.6566751640274361 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.000194133673840128, 'max_features': 0.8573163807699049, 'min_impurity_decrease': 3.642463041621773e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:25,946] Trial 82 finished with value: 0.6568154733692743 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00028171597529728167, 'max_features': 0.8326232000808443, 'min_impurity_decrease': 1.4309840910253914e-07}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,015] Trial 83 finished with value: 0.6551823073636419 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.000130074727397071, 'max_features': 0.783969384328462, 'min_impurity_decrease': 1.8613993930819036e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,092] Trial 84 finished with value: 0.6593394769385562 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0001624098805250173, 'max_features': 0.8775446639140593, 'min_impurity_decrease': 6.170816393299652e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,173] Trial 85 finished with value: 0.6550983227307742 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00016263050850903843, 'max_features': 0.8816415283985254, 'min_impurity_decrease': 4.9295158406159104e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,237] Trial 86 finished with value: 0.643767964170641 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0002213233854452968, 'max_features': 0.7476718070282932, 'min_impurity_decrease': 1.4492136966324322e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,312] Trial 87 finished with value: 0.6574068784229544 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00015386517503931544, 'max_features': 0.8070690316106988, 'min_impurity_decrease': 6.03739379002804e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,385] Trial 88 finished with value: 0.652824362331699 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0001575715909637756, 'max_features': 0.7977920748153324, 'min_impurity_decrease': 7.270630945566724e-08}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,421] Trial 89 finished with value: 0.5716591421026768 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0002548018121016168, 'max_features': 0.7621405631321065, 'min_impurity_decrease': 0.06849659180480638}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,500] Trial 90 finished with value: 0.6508625575983089 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00037290284191765933, 'max_features': 0.7088671766345228, 'min_impurity_decrease': 2.1553322614924623e-07}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,574] Trial 91 finished with value: 0.6587292921852893 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00012449367411437976, 'max_features': 0.8137561052300087, 'min_impurity_decrease': 1.1376713080447048e-07}. Best is trial 75 with value: 0.6597011616896998.\n",
      "[I 2025-02-04 19:35:26,649] Trial 92 finished with value: 0.6618327169610131 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00010406727248534515, 'max_features': 0.8717688349036992, 'min_impurity_decrease': 1.4546995718217156e-07}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:26,724] Trial 93 finished with value: 0.6562413476638238 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00010390388601656982, 'max_features': 0.8665829651957564, 'min_impurity_decrease': 1.2991891877922352e-07}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:26,804] Trial 94 finished with value: 0.6543502553464563 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00019281894193280585, 'max_features': 0.934896531523902, 'min_impurity_decrease': 2.760753805174393e-07}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:26,874] Trial 95 finished with value: 0.651343507553865 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00011900595616173975, 'max_features': 0.8280637918075664, 'min_impurity_decrease': 0.00027582726638568155}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:26,951] Trial 96 finished with value: 0.6565877064000983 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00027863127589572944, 'max_features': 0.8907833156503583, 'min_impurity_decrease': 4.024848508788897e-08}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:27,026] Trial 97 finished with value: 0.6553576455122302 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00017493587065652816, 'max_features': 0.8571945887710339, 'min_impurity_decrease': 1.1284069939288811e-07}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:27,078] Trial 98 finished with value: 0.6456330324550562 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00013466851396628875, 'max_features': 0.41008495174954995, 'min_impurity_decrease': 2.934546229879303e-08}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:27,141] Trial 99 finished with value: 0.6453739328690875 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.00022088501208731602, 'max_features': 0.7733892360989884, 'min_impurity_decrease': 1.8075227547041655e-07}. Best is trial 92 with value: 0.6618327169610131.\n",
      "[I 2025-02-04 19:35:27,170] A new study created in memory with name: no-name-c3a790d7-2886-49b2-a674-b0c98c5bc486\n",
      "[I 2025-02-04 19:35:31,182] Trial 0 finished with value: 0.6660399320757248 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.002579706592624233, 'max_features': 0.7613408187483958, 'min_impurity_decrease': 0.0003363631240798668}. Best is trial 0 with value: 0.6660399320757248.\n",
      "[I 2025-02-04 19:35:34,290] Trial 1 finished with value: 0.6577228794393449 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.007422837407422639, 'max_features': 0.7171571867169797, 'min_impurity_decrease': 0.0011191801386623015}. Best is trial 0 with value: 0.6660399320757248.\n",
      "[I 2025-02-04 19:35:36,303] Trial 2 finished with value: 0.66454441892812 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.00016799365242949508, 'max_features': 0.31704805582447954, 'min_impurity_decrease': 1.06443695435346e-05}. Best is trial 0 with value: 0.6660399320757248.\n",
      "[I 2025-02-04 19:35:37,763] Trial 3 finished with value: 0.5834350327158511 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.062249794675984994, 'max_features': 0.4547523818051576, 'min_impurity_decrease': 2.4683042762781045e-07}. Best is trial 0 with value: 0.6660399320757248.\n",
      "[I 2025-02-04 19:35:39,347] Trial 4 finished with value: 0.5965643931784882 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.000674866527610678, 'max_features': 0.5600638709635629, 'min_impurity_decrease': 0.01938721110423989}. Best is trial 0 with value: 0.6660399320757248.\n",
      "[I 2025-02-04 19:35:42,125] Trial 5 finished with value: 0.6084940282521488 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.035568702158863444, 'max_features': 0.8353966468022334, 'min_impurity_decrease': 1.3156036215323096e-05}. Best is trial 0 with value: 0.6660399320757248.\n",
      "[I 2025-02-04 19:35:47,382] Trial 6 finished with value: 0.6721576132861719 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0009366437889012607, 'max_features': 0.8246431904902858, 'min_impurity_decrease': 1.0237701583923564e-07}. Best is trial 6 with value: 0.6721576132861719.\n",
      "[I 2025-02-04 19:35:49,659] Trial 7 finished with value: 0.6631691072295306 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.002651906547152191, 'max_features': 0.355909599209069, 'min_impurity_decrease': 8.238497927323628e-06}. Best is trial 6 with value: 0.6721576132861719.\n",
      "[I 2025-02-04 19:35:54,633] Trial 8 finished with value: 0.6703631353881224 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0014053975622865594, 'max_features': 0.8343620464668492, 'min_impurity_decrease': 8.839895205501905e-05}. Best is trial 6 with value: 0.6721576132861719.\n",
      "[I 2025-02-04 19:35:57,782] Trial 9 finished with value: 0.632664008492617 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.01926306582250902, 'max_features': 0.8051348154681676, 'min_impurity_decrease': 5.299780886371612e-05}. Best is trial 6 with value: 0.6721576132861719.\n",
      "[I 2025-02-04 19:36:02,117] Trial 10 finished with value: 0.6619316548729123 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0001186337007109744, 'max_features': 0.9772392328583983, 'min_impurity_decrease': 1.518647328142794e-08}. Best is trial 6 with value: 0.6721576132861719.\n",
      "[I 2025-02-04 19:36:03,278] Trial 11 finished with value: 0.11767086558405422 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0006484318478850398, 'max_features': 0.9721082250195683, 'min_impurity_decrease': 0.48266031636670426}. Best is trial 6 with value: 0.6721576132861719.\n",
      "[I 2025-02-04 19:36:06,815] Trial 12 finished with value: 0.6686293930125572 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.0006439834321371468, 'max_features': 0.6089891735118346, 'min_impurity_decrease': 2.727117031557889e-07}. Best is trial 6 with value: 0.6721576132861719.\n",
      "[I 2025-02-04 19:36:11,016] Trial 13 finished with value: 0.6727355525357992 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0014198702400035651, 'max_features': 0.6647281707820896, 'min_impurity_decrease': 1.1022222461712768e-08}. Best is trial 13 with value: 0.6727355525357992.\n",
      "[I 2025-02-04 19:36:12,189] Trial 14 finished with value: 0.6181970470418987 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.008058559604768124, 'max_features': 0.15447486505971525, 'min_impurity_decrease': 2.222502350982024e-08}. Best is trial 13 with value: 0.6727355525357992.\n",
      "[I 2025-02-04 19:36:16,330] Trial 15 finished with value: 0.6726478880148498 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.000290599552639989, 'max_features': 0.6618131052088805, 'min_impurity_decrease': 4.2185235598066415e-07}. Best is trial 13 with value: 0.6727355525357992.\n",
      "[I 2025-02-04 19:36:20,314] Trial 16 finished with value: 0.6743196432501191 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00025752483195314914, 'max_features': 0.6242338756723337, 'min_impurity_decrease': 1.068991136813951e-06}. Best is trial 16 with value: 0.6743196432501191.\n",
      "[I 2025-02-04 19:36:23,223] Trial 17 finished with value: 0.670560235044975 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.00039195517631421, 'max_features': 0.46798157184743466, 'min_impurity_decrease': 1.6967094176066862e-06}. Best is trial 16 with value: 0.6743196432501191.\n",
      "[I 2025-02-04 19:36:26,513] Trial 18 finished with value: 0.6727484797371132 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0002485722680050379, 'max_features': 0.49906010291621483, 'min_impurity_decrease': 3.592735780781845e-08}. Best is trial 16 with value: 0.6743196432501191.\n",
      "[I 2025-02-04 19:36:29,391] Trial 19 finished with value: 0.6703215650920269 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.00021647719359150076, 'max_features': 0.462826615997522, 'min_impurity_decrease': 1.684060415496134e-06}. Best is trial 16 with value: 0.6743196432501191.\n",
      "[I 2025-02-04 19:36:31,433] Trial 20 finished with value: 0.6706978317521699 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0001280639150410495, 'max_features': 0.266003763219707, 'min_impurity_decrease': 1.4116758399911e-06}. Best is trial 16 with value: 0.6743196432501191.\n",
      "[I 2025-02-04 19:36:35,274] Trial 21 finished with value: 0.667685557972172 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0014979459278772129, 'max_features': 0.625075027216249, 'min_impurity_decrease': 1.096249388022695e-08}. Best is trial 16 with value: 0.6743196432501191.\n",
      "[I 2025-02-04 19:36:38,922] Trial 22 finished with value: 0.6769882979029491 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00035387124457338657, 'max_features': 0.5216999581251804, 'min_impurity_decrease': 6.25458984618835e-08}. Best is trial 22 with value: 0.6769882979029491.\n",
      "[I 2025-02-04 19:36:42,213] Trial 23 finished with value: 0.6734257937283017 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00036376663847824625, 'max_features': 0.4967545132204564, 'min_impurity_decrease': 6.763832782161104e-08}. Best is trial 22 with value: 0.6769882979029491.\n",
      "[I 2025-02-04 19:36:44,556] Trial 24 finished with value: 0.6658612736443594 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.0003970639626858961, 'max_features': 0.39474191534755587, 'min_impurity_decrease': 9.623635763327001e-08}. Best is trial 22 with value: 0.6769882979029491.\n",
      "[I 2025-02-04 19:36:47,811] Trial 25 finished with value: 0.6695414013700414 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.00043554847291726433, 'max_features': 0.542022929216517, 'min_impurity_decrease': 8.085443004078162e-07}. Best is trial 22 with value: 0.6769882979029491.\n",
      "[I 2025-02-04 19:36:51,718] Trial 26 finished with value: 0.6781747018418147 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00010714200515835113, 'max_features': 0.5609228100404773, 'min_impurity_decrease': 9.847262351097671e-08}. Best is trial 26 with value: 0.6781747018418147.\n",
      "[I 2025-02-04 19:36:54,113] Trial 27 finished with value: 0.6485103933338129 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00015402841868850927, 'max_features': 0.5866364307832875, 'min_impurity_decrease': 0.003664113492885845}. Best is trial 26 with value: 0.6781747018418147.\n",
      "[I 2025-02-04 19:36:56,159] Trial 28 finished with value: 0.6716001956669132 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00011565956451716202, 'max_features': 0.24382467325777202, 'min_impurity_decrease': 6.1259939348470055e-06}. Best is trial 26 with value: 0.6781747018418147.\n",
      "[I 2025-02-04 19:37:00,783] Trial 29 finished with value: 0.6767153995650169 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00021813706516953798, 'max_features': 0.7484165255995249, 'min_impurity_decrease': 1.410833633208405e-07}. Best is trial 26 with value: 0.6781747018418147.\n",
      "[I 2025-02-04 19:37:04,542] Trial 30 finished with value: 0.6645588444908307 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.004406538650415139, 'max_features': 0.7476100834995322, 'min_impurity_decrease': 0.00031253605723091177}. Best is trial 26 with value: 0.6781747018418147.\n",
      "[I 2025-02-04 19:37:08,919] Trial 31 finished with value: 0.6761099075351025 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0002036287725593843, 'max_features': 0.694450546851559, 'min_impurity_decrease': 1.3886155833118054e-07}. Best is trial 26 with value: 0.6781747018418147.\n",
      "[I 2025-02-04 19:37:13,306] Trial 32 finished with value: 0.6757227872770044 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00010060151320829266, 'max_features': 0.7023023957466855, 'min_impurity_decrease': 1.7365272317023832e-07}. Best is trial 26 with value: 0.6781747018418147.\n",
      "[I 2025-02-04 19:37:19,192] Trial 33 finished with value: 0.6811214595199285 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0001857952581962462, 'max_features': 0.9104468599245602, 'min_impurity_decrease': 4.952284302240182e-08}. Best is trial 33 with value: 0.6811214595199285.\n",
      "[I 2025-02-04 19:37:25,178] Trial 34 finished with value: 0.6821189243239698 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0001799177904596647, 'max_features': 0.9075132380999819, 'min_impurity_decrease': 3.416696762391615e-08}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:37:30,965] Trial 35 finished with value: 0.6810392886361502 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00016394392229866867, 'max_features': 0.8865546209759252, 'min_impurity_decrease': 3.261743204218629e-08}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:37:36,928] Trial 36 finished with value: 0.6807929150841795 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00015588294269185934, 'max_features': 0.9138554120637213, 'min_impurity_decrease': 4.0135482921311506e-08}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:37:40,447] Trial 37 finished with value: 0.6563419004579326 and parameters: {'max_depth': 5, 'min_samples_leaf': 0.0009116892764003997, 'max_features': 0.8998438921869533, 'min_impurity_decrease': 2.9100719575854793e-08}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:37:42,296] Trial 38 finished with value: 0.5782373564270781 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.09819460028757113, 'max_features': 0.9051566171174623, 'min_impurity_decrease': 5.701712594117418e-07}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:37:46,345] Trial 39 finished with value: 0.6629594129325803 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.00016302165919758845, 'max_features': 0.9069835590528116, 'min_impurity_decrease': 3.843237826691642e-06}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:37:52,515] Trial 40 finished with value: 0.6763130522739328 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0005531512085902151, 'max_features': 0.9972534496217306, 'min_impurity_decrease': 3.0053273318639428e-05}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:37:58,624] Trial 41 finished with value: 0.6791322413687199 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00015245481666037228, 'max_features': 0.9383562999214253, 'min_impurity_decrease': 3.653083139439599e-08}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:38:04,392] Trial 42 finished with value: 0.6818003564007928 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00016856460500169423, 'max_features': 0.8716202126151685, 'min_impurity_decrease': 3.509469635074613e-08}. Best is trial 34 with value: 0.6821189243239698.\n",
      "[I 2025-02-04 19:38:10,093] Trial 43 finished with value: 0.6823952187052201 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0001753290025176309, 'max_features': 0.8669903759730354, 'min_impurity_decrease': 4.263978844792576e-08}. Best is trial 43 with value: 0.6823952187052201.\n",
      "[I 2025-02-04 19:38:15,673] Trial 44 finished with value: 0.6773771754373864 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.000307170843226551, 'max_features': 0.8560146300461079, 'min_impurity_decrease': 3.252238842759273e-07}. Best is trial 43 with value: 0.6823952187052201.\n",
      "[I 2025-02-04 19:38:17,269] Trial 45 finished with value: 0.5782373564270781 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0009253742810898314, 'max_features': 0.785647943688762, 'min_impurity_decrease': 0.04428268249316982}. Best is trial 43 with value: 0.6823952187052201.\n",
      "[I 2025-02-04 19:38:20,881] Trial 46 finished with value: 0.6360481646674042 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.016167587608505365, 'max_features': 0.866185145903496, 'min_impurity_decrease': 1.6763108439351054e-08}. Best is trial 43 with value: 0.6823952187052201.\n",
      "[I 2025-02-04 19:38:24,515] Trial 47 finished with value: 0.661232982829512 and parameters: {'max_depth': 6, 'min_samples_leaf': 0.0005177807855990584, 'max_features': 0.7913715372879823, 'min_impurity_decrease': 2.8000963545962965e-07}. Best is trial 43 with value: 0.6823952187052201.\n",
      "[I 2025-02-04 19:38:30,905] Trial 48 finished with value: 0.6826343187416652 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00021390557065255724, 'max_features': 0.9574388303394594, 'min_impurity_decrease': 1.15828897547245e-08}. Best is trial 48 with value: 0.6826343187416652.\n",
      "[I 2025-02-04 19:38:36,660] Trial 49 finished with value: 0.6734347310395309 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0002099848057805702, 'max_features': 0.9497722186689872, 'min_impurity_decrease': 1.020760011472085e-08}. Best is trial 48 with value: 0.6826343187416652.\n",
      "[I 2025-02-04 19:38:41,465] Trial 50 finished with value: 0.6666343610987006 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.003914421815649635, 'max_features': 0.8365764671095068, 'min_impurity_decrease': 2.1462545129404293e-08}. Best is trial 48 with value: 0.6826343187416652.\n",
      "[I 2025-02-04 19:38:47,142] Trial 51 finished with value: 0.6780355539184167 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00028182479683363886, 'max_features': 0.874154442488512, 'min_impurity_decrease': 5.4785347705004215e-08}. Best is trial 48 with value: 0.6826343187416652.\n",
      "[I 2025-02-04 19:38:53,471] Trial 52 finished with value: 0.6827815914578229 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00018141432435076767, 'max_features': 0.9689707325998492, 'min_impurity_decrease': 3.313066521571167e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:38:59,692] Trial 53 finished with value: 0.6819338218871226 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00018605016470750818, 'max_features': 0.950965191279063, 'min_impurity_decrease': 2.0923713864612937e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:06,051] Trial 54 finished with value: 0.6804302002094108 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00012989310521833801, 'max_features': 0.9598661461715532, 'min_impurity_decrease': 1.7994636851687032e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:12,030] Trial 55 finished with value: 0.6764144884506488 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0002652791675718479, 'max_features': 0.9799990185748582, 'min_impurity_decrease': 2.057891732690581e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:18,140] Trial 56 finished with value: 0.6809497753149896 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0001339392831555508, 'max_features': 0.9441299639804844, 'min_impurity_decrease': 7.17151653801195e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:23,196] Trial 57 finished with value: 0.6696917180489996 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.0007790318708861143, 'max_features': 0.8403479278641917, 'min_impurity_decrease': 6.044831322110217e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:28,700] Trial 58 finished with value: 0.6766790202320436 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0005168569165280764, 'max_features': 0.8133852099253425, 'min_impurity_decrease': 2.7818319820000207e-06}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:33,591] Trial 59 finished with value: 0.6621505807417783 and parameters: {'max_depth': 7, 'min_samples_leaf': 0.0018224959555399587, 'max_features': 0.9840433556220547, 'min_impurity_decrease': 1.2870998640305295e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:39,762] Trial 60 finished with value: 0.6750198704578145 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.000303430011380103, 'max_features': 0.9991230844554803, 'min_impurity_decrease': 2.4826012295432328e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:45,941] Trial 61 finished with value: 0.6819151504463293 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00018772430108414243, 'max_features': 0.9256569627046983, 'min_impurity_decrease': 5.7894273645823076e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:52,078] Trial 62 finished with value: 0.6807648666953144 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002270272366069383, 'max_features': 0.9377233194130724, 'min_impurity_decrease': 1.0645175276640717e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:39:58,138] Trial 63 finished with value: 0.680825392452916 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00010281943189068833, 'max_features': 0.9254668193390417, 'min_impurity_decrease': 3.651226833947213e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:03,704] Trial 64 finished with value: 0.6793161348777647 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0004205329443752711, 'max_features': 0.8572192166607872, 'min_impurity_decrease': 7.696181969081312e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:08,926] Trial 65 finished with value: 0.6814790148979327 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00017876319540755501, 'max_features': 0.7669874338569138, 'min_impurity_decrease': 1.789864174248e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:14,855] Trial 66 finished with value: 0.6729602333943001 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00034754209076178405, 'max_features': 0.9599057561005282, 'min_impurity_decrease': 1.757986337950307e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:20,618] Trial 67 finished with value: 0.6811703931803789 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002447864429382731, 'max_features': 0.8807197747879376, 'min_impurity_decrease': 4.887964955467328e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:26,025] Trial 68 finished with value: 0.6809950297321262 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0001314545842972761, 'max_features': 0.8146577218359741, 'min_impurity_decrease': 1.0545656040151824e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:31,619] Trial 69 finished with value: 0.6745378041336524 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00017671247963922272, 'max_features': 0.9697197496317229, 'min_impurity_decrease': 0.0002077045990588125}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:32,773] Trial 70 finished with value: 0.11767086558405422 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.007869849482270364, 'max_features': 0.9264572878950791, 'min_impurity_decrease': 0.4435745295450755}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:37,910] Trial 71 finished with value: 0.6811145159606503 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00019227879139554718, 'max_features': 0.7770083906559114, 'min_impurity_decrease': 1.8483241874316705e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:41,553] Trial 72 finished with value: 0.6605281434480386 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00013303604374343341, 'max_features': 0.896779977129778, 'min_impurity_decrease': 0.0022331309143302455}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:46,435] Trial 73 finished with value: 0.6821013443317385 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00019553541229018291, 'max_features': 0.7313588782124355, 'min_impurity_decrease': 3.0088707734260623e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:52,087] Trial 74 finished with value: 0.6783298713870748 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00033076417252196025, 'max_features': 0.8409423668565468, 'min_impurity_decrease': 2.8979393143297127e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:40:57,033] Trial 75 finished with value: 0.6818094089404582 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002467278355691953, 'max_features': 0.73106698732612, 'min_impurity_decrease': 1.0941536754129681e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:03,308] Trial 76 finished with value: 0.6819792976395503 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002520272590873618, 'max_features': 0.9621547906960378, 'min_impurity_decrease': 1.0511163886776161e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:09,064] Trial 77 finished with value: 0.6728936516915182 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00043895805036045694, 'max_features': 0.9623276166284751, 'min_impurity_decrease': 7.316032769243845e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:13,565] Trial 78 finished with value: 0.6797732432097812 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00022339178880310622, 'max_features': 0.6578682307023539, 'min_impurity_decrease': 9.183189638500316e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:19,677] Trial 79 finished with value: 0.6797993842613458 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00011425359504871426, 'max_features': 0.9249129372351946, 'min_impurity_decrease': 4.902512819593177e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:25,112] Trial 80 finished with value: 0.6653906760858729 and parameters: {'max_depth': 8, 'min_samples_leaf': 0.0006880953338789218, 'max_features': 0.9786451112400257, 'min_impurity_decrease': 4.860388734378437e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:28,196] Trial 81 finished with value: 0.6756486328621873 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002637286456198217, 'max_features': 0.4152565847908952, 'min_impurity_decrease': 1.1647314206058611e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:33,245] Trial 82 finished with value: 0.68090363236286 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00022815355074289425, 'max_features': 0.7487129117353745, 'min_impurity_decrease': 1.060012354887673e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:38,178] Trial 83 finished with value: 0.6813563937841168 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00015191652572620558, 'max_features': 0.7214595304037553, 'min_impurity_decrease': 2.5623474340643977e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:41,412] Trial 84 finished with value: 0.6310048473763298 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.026772324048775734, 'max_features': 0.8890033059655568, 'min_impurity_decrease': 2.4498399632546204e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:47,777] Trial 85 finished with value: 0.6793091496524547 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00019499089549645447, 'max_features': 0.9954545735364607, 'min_impurity_decrease': 2.3542848137789694e-05}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:53,918] Trial 86 finished with value: 0.6781135529197478 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0003520168412836955, 'max_features': 0.9397143997924865, 'min_impurity_decrease': 1.8600990139624993e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:41:59,903] Trial 87 finished with value: 0.6794310707455736 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002881072485304365, 'max_features': 0.9114877217957413, 'min_impurity_decrease': 5.1037166354967026e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:04,073] Trial 88 finished with value: 0.6776225348373034 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00014373552605604465, 'max_features': 0.6434179603488224, 'min_impurity_decrease': 1.4667733268743878e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:09,580] Trial 89 finished with value: 0.6780339547048863 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0004946051643764936, 'max_features': 0.8560242110044697, 'min_impurity_decrease': 3.080534727789669e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:12,903] Trial 90 finished with value: 0.6462063357161575 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.012344431775514421, 'max_features': 0.7029877845681999, 'min_impurity_decrease': 8.374325336830142e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:18,638] Trial 91 finished with value: 0.6807074014531866 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.000175236786476892, 'max_features': 0.8720608826853643, 'min_impurity_decrease': 4.0989851667596827e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:22,623] Trial 92 finished with value: 0.6781622722257654 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00020067667627882122, 'max_features': 0.583167454465128, 'min_impurity_decrease': 1.854317798399586e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:27,894] Trial 93 finished with value: 0.6793341664333985 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0002646661362994633, 'max_features': 0.80139879637112, 'min_impurity_decrease': 6.418302268005709e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:34,101] Trial 94 finished with value: 0.6800737421070214 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00012314628596004964, 'max_features': 0.9510844322710174, 'min_impurity_decrease': 1.2449688701751264e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:40,104] Trial 95 finished with value: 0.6818775156952795 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00016676438990241734, 'max_features': 0.886593124898298, 'min_impurity_decrease': 2.9131030004469354e-08}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:46,088] Trial 96 finished with value: 0.6820108021884824 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00011313312582861791, 'max_features': 0.9022898631215495, 'min_impurity_decrease': 2.9868687150859565e-07}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:52,087] Trial 97 finished with value: 0.6807411480678808 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.0001110831732024563, 'max_features': 0.9188426069315295, 'min_impurity_decrease': 1.4444201239974107e-06}. Best is trial 52 with value: 0.6827815914578229.\n",
      "[I 2025-02-04 19:42:57,979] Trial 98 finished with value: 0.6833315156868521 and parameters: {'max_depth': 10, 'min_samples_leaf': 0.00015909092176659916, 'max_features': 0.8868868010298527, 'min_impurity_decrease': 2.9366243266146103e-07}. Best is trial 98 with value: 0.6833315156868521.\n",
      "[I 2025-02-04 19:43:03,961] Trial 99 finished with value: 0.6756382580296688 and parameters: {'max_depth': 9, 'min_samples_leaf': 0.00014504594604455593, 'max_features': 0.9782172060071783, 'min_impurity_decrease': 3.653031008769409e-07}. Best is trial 98 with value: 0.6833315156868521.\n",
      "[I 2025-02-04 19:43:06,848] A new study created in memory with name: no-name-5e3d8401-feea-4c45-acb4-a4f3d68594f5\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:08,276] Trial 0 finished with value: 0.6559448547724588 and parameters: {'learning_rate': 1.2855481169174391e-07, 'min_split_gain': 0.051782507859789675, 'reg_alpha': 2.290703577941546e-06, 'reg_lambda': 2.2243524847666998e-06}. Best is trial 0 with value: 0.6559448547724588.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:09,694] Trial 1 finished with value: 0.6705279892337205 and parameters: {'learning_rate': 0.014610315938991637, 'min_split_gain': 0.1170607467121915, 'reg_alpha': 0.7539420081528714, 'reg_lambda': 7.880856526700203e-06}. Best is trial 1 with value: 0.6705279892337205.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:11,123] Trial 2 finished with value: 0.6571257716601596 and parameters: {'learning_rate': 0.00020290077361219378, 'min_split_gain': 0.01408787165555207, 'reg_alpha': 0.0017828970074476626, 'reg_lambda': 3.4103733151357836e-05}. Best is trial 1 with value: 0.6705279892337205.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:12,656] Trial 3 finished with value: 0.6554194469236219 and parameters: {'learning_rate': 0.002135123497339992, 'min_split_gain': 0.0007184421536798806, 'reg_alpha': 7.37479469087633e-05, 'reg_lambda': 9.92893875659685e-07}. Best is trial 1 with value: 0.6705279892337205.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:13,983] Trial 4 finished with value: 0.7029068572245988 and parameters: {'learning_rate': 0.16800680185393424, 'min_split_gain': 0.004304419181188643, 'reg_alpha': 3.2807639400762635e-05, 'reg_lambda': 0.0066093117927818615}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001607 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:15,384] Trial 5 finished with value: 0.6564745982331215 and parameters: {'learning_rate': 4.656067498926319e-05, 'min_split_gain': 0.0010096415382741782, 'reg_alpha': 1.177188387392638e-07, 'reg_lambda': 1.5685477617668546e-05}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:16,847] Trial 6 finished with value: 0.6564745982331215 and parameters: {'learning_rate': 7.087670962430928e-05, 'min_split_gain': 0.00437614228681317, 'reg_alpha': 3.39602496766567e-08, 'reg_lambda': 1.2843754540750401e-08}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:18,287] Trial 7 finished with value: 0.6529180258045271 and parameters: {'learning_rate': 1.1277374269412797e-08, 'min_split_gain': 0.2795540924825243, 'reg_alpha': 0.005618866070134586, 'reg_lambda': 2.8089565294327287e-07}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:19,736] Trial 8 finished with value: 0.6549152728124711 and parameters: {'learning_rate': 0.0003071101599427852, 'min_split_gain': 0.00019091810179546094, 'reg_alpha': 2.8213022773770395e-07, 'reg_lambda': 1.6691483058186116e-08}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:21,191] Trial 9 finished with value: 0.6556668846257496 and parameters: {'learning_rate': 0.0019496922514298646, 'min_split_gain': 5.2403965251692675e-06, 'reg_alpha': 1.9928408115621485e-07, 'reg_lambda': 0.0003735253361093897}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001599 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:22,558] Trial 10 finished with value: 0.6999574540994787 and parameters: {'learning_rate': 0.3396969602189236, 'min_split_gain': 1.9226145414781687e-08, 'reg_alpha': 2.3015087878538904e-05, 'reg_lambda': 0.10891271217810598}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:23,905] Trial 11 finished with value: 0.6719864071780327 and parameters: {'learning_rate': 0.7593807094496486, 'min_split_gain': 1.91635998849205e-07, 'reg_alpha': 2.323211483195755e-05, 'reg_lambda': 0.12811085593548854}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:25,252] Trial 12 finished with value: 0.6967962618187635 and parameters: {'learning_rate': 0.37530174677211214, 'min_split_gain': 1.3787680234973323e-08, 'reg_alpha': 0.0009952253897196753, 'reg_lambda': 0.08319381416815277}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001746 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:26,665] Trial 13 finished with value: 0.6967156719031208 and parameters: {'learning_rate': 0.06584506886867857, 'min_split_gain': 8.455758902213367e-06, 'reg_alpha': 5.126260327958297e-06, 'reg_lambda': 0.003288391885071523}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:28,093] Trial 14 finished with value: 0.6564745982331215 and parameters: {'learning_rate': 1.64518104387684e-06, 'min_split_gain': 1.504182801142519e-05, 'reg_alpha': 0.0005220574442332219, 'reg_lambda': 0.006641543717770372}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:29,497] Trial 15 finished with value: 0.6921218036233842 and parameters: {'learning_rate': 0.04100838936712594, 'min_split_gain': 2.9767191053342327e-07, 'reg_alpha': 0.023613448361979244, 'reg_lambda': 0.9663018374427835}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001639 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:30,835] Trial 16 finished with value: 0.7006039536709728 and parameters: {'learning_rate': 0.2486976374823787, 'min_split_gain': 1.0151629783689285e-08, 'reg_alpha': 4.516163650020142e-06, 'reg_lambda': 0.000746177524158863}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:32,252] Trial 17 finished with value: 0.6652680809107763 and parameters: {'learning_rate': 0.008924509999511812, 'min_split_gain': 6.648085505463142e-05, 'reg_alpha': 1.142206788043536e-06, 'reg_lambda': 0.0003899912509197427}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:33,643] Trial 18 finished with value: 0.6567155191246671 and parameters: {'learning_rate': 6.505213635713406e-06, 'min_split_gain': 0.8937359847252898, 'reg_alpha': 0.00014245538007925818, 'reg_lambda': 0.0033066943590162757}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:35,025] Trial 19 finished with value: 0.7004818169202496 and parameters: {'learning_rate': 0.09529776945423377, 'min_split_gain': 5.146063153540811e-07, 'reg_alpha': 0.03575324176845477, 'reg_lambda': 0.0002523829374165235}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:36,419] Trial 20 finished with value: 0.6560902974547307 and parameters: {'learning_rate': 0.0018318578147187752, 'min_split_gain': 0.005950347663420224, 'reg_alpha': 1.3865873064426652e-08, 'reg_lambda': 0.011161976356644544}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:37,843] Trial 21 finished with value: 0.7019905456834543 and parameters: {'learning_rate': 0.09349425510711087, 'min_split_gain': 9.055723856048387e-08, 'reg_alpha': 0.14070428268853843, 'reg_lambda': 0.00021417414625314558}. Best is trial 4 with value: 0.7029068572245988.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:39,254] Trial 22 finished with value: 0.7051232263293153 and parameters: {'learning_rate': 0.1440999031554747, 'min_split_gain': 5.659617944094974e-08, 'reg_alpha': 0.8094248952540165, 'reg_lambda': 0.000883343073215532}. Best is trial 22 with value: 0.7051232263293153.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:40,709] Trial 23 finished with value: 0.6727676064617037 and parameters: {'learning_rate': 0.016407525747203246, 'min_split_gain': 2.2197960942705478e-06, 'reg_alpha': 0.7728990672754645, 'reg_lambda': 3.9024196825071125e-05}. Best is trial 22 with value: 0.7051232263293153.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:42,121] Trial 24 finished with value: 0.7057797695356766 and parameters: {'learning_rate': 0.10174895743601302, 'min_split_gain': 7.495108727730083e-08, 'reg_alpha': 0.05458170962841965, 'reg_lambda': 0.0010541840293168425}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001583 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:43,553] Trial 25 finished with value: 0.6628675464919916 and parameters: {'learning_rate': 0.005484895623221368, 'min_split_gain': 1.5362521257340048e-06, 'reg_alpha': 0.12511968157553816, 'reg_lambda': 0.0013052578117237652}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001459 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:44,996] Trial 26 finished with value: 0.6553079236967536 and parameters: {'learning_rate': 0.0008165551194648955, 'min_split_gain': 6.474542207658823e-08, 'reg_alpha': 0.0090701939713255, 'reg_lambda': 0.029815025646799147}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001571 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:46,386] Trial 27 finished with value: 0.6680188941351566 and parameters: {'learning_rate': 0.8076148545123437, 'min_split_gain': 2.7218588714951083e-05, 'reg_alpha': 0.2354130470153082, 'reg_lambda': 0.0203341553584536}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:47,815] Trial 28 finished with value: 0.6881481687569767 and parameters: {'learning_rate': 0.0368037376570246, 'min_split_gain': 0.0003803201424255026, 'reg_alpha': 0.02876940713369455, 'reg_lambda': 0.001357391703679166}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:49,240] Trial 29 finished with value: 0.6564745982331215 and parameters: {'learning_rate': 8.730723803126146e-07, 'min_split_gain': 0.02446290667840466, 'reg_alpha': 0.00017398180251908284, 'reg_lambda': 9.388778171121617e-05}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:50,638] Trial 30 finished with value: 0.7006709068663352 and parameters: {'learning_rate': 0.20821881172307397, 'min_split_gain': 0.001704064794792865, 'reg_alpha': 0.003247279517167112, 'reg_lambda': 0.7955196355087003}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:52,074] Trial 31 finished with value: 0.7014423626450826 and parameters: {'learning_rate': 0.07333739030614699, 'min_split_gain': 4.96171270572255e-08, 'reg_alpha': 0.13149477913692612, 'reg_lambda': 0.00011322313037291592}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:53,520] Trial 32 finished with value: 0.6745094851664111 and parameters: {'learning_rate': 0.01845687747150744, 'min_split_gain': 8.550989410933701e-08, 'reg_alpha': 0.3138724144733094, 'reg_lambda': 4.265084966692457e-06}. Best is trial 24 with value: 0.7057797695356766.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:54,925] Trial 33 finished with value: 0.7066683865041478 and parameters: {'learning_rate': 0.1258994936753425, 'min_split_gain': 1.1191138285281142e-06, 'reg_alpha': 0.06377858348616366, 'reg_lambda': 0.00012278609109352733}. Best is trial 33 with value: 0.7066683865041478.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:56,369] Trial 34 finished with value: 0.6661671809753124 and parameters: {'learning_rate': 0.00980660367676898, 'min_split_gain': 1.1400763663961045e-06, 'reg_alpha': 0.9458867142045472, 'reg_lambda': 0.0038610594914804813}. Best is trial 33 with value: 0.7066683865041478.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001489 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:57,770] Trial 35 finished with value: 0.6614645776169277 and parameters: {'learning_rate': 0.8530125556987386, 'min_split_gain': 7.005583734111073e-07, 'reg_alpha': 0.05677612617673736, 'reg_lambda': 3.781055278938672e-05}. Best is trial 33 with value: 0.7066683865041478.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:43:59,166] Trial 36 finished with value: 0.7100064700948808 and parameters: {'learning_rate': 0.16070864915912128, 'min_split_gain': 2.992026339205254e-08, 'reg_alpha': 0.016593336897950957, 'reg_lambda': 0.0016370414555182439}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:00,613] Trial 37 finished with value: 0.6608968602499911 and parameters: {'learning_rate': 0.0035790740131622645, 'min_split_gain': 1.5907175537901603e-07, 'reg_alpha': 0.012118822126708376, 'reg_lambda': 1.3884674972250436e-05}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:02,057] Trial 38 finished with value: 0.6818931254520472 and parameters: {'learning_rate': 0.025370832547205013, 'min_split_gain': 3.1063965073630316e-08, 'reg_alpha': 0.002016467486402515, 'reg_lambda': 1.0617928823614035e-06}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:03,499] Trial 39 finished with value: 0.6555826377770783 and parameters: {'learning_rate': 2.0933647385062734e-05, 'min_split_gain': 2.954477818880008e-06, 'reg_alpha': 0.3925423872887113, 'reg_lambda': 0.0008780736139180217}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:04,943] Trial 40 finished with value: 0.6544641437517665 and parameters: {'learning_rate': 0.0002709758065294899, 'min_split_gain': 3.4057941506776284e-07, 'reg_alpha': 0.06975932296144158, 'reg_lambda': 0.0017635560623022488}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:06,345] Trial 41 finished with value: 0.7037661475199283 and parameters: {'learning_rate': 0.22158065354907053, 'min_split_gain': 0.06393745145074056, 'reg_alpha': 2.880757436809721e-05, 'reg_lambda': 0.011176910638358559}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:07,737] Trial 42 finished with value: 0.7028640347066536 and parameters: {'learning_rate': 0.16568412815723296, 'min_split_gain': 0.06975019996048012, 'reg_alpha': 0.00044776783612147686, 'reg_lambda': 0.00011211244785689331}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:09,117] Trial 43 finished with value: 0.6855676490625339 and parameters: {'learning_rate': 0.42755943084233594, 'min_split_gain': 2.516880138964926e-08, 'reg_alpha': 0.012385180482166637, 'reg_lambda': 0.033611864026822376}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:10,593] Trial 44 finished with value: 0.6564745982331215 and parameters: {'learning_rate': 3.199321041411798e-08, 'min_split_gain': 1.1098310559712554e-07, 'reg_alpha': 6.668511152380963e-05, 'reg_lambda': 0.0005287888376151536}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:12,019] Trial 45 finished with value: 0.7033537775476129 and parameters: {'learning_rate': 0.13230762988457317, 'min_split_gain': 3.614523448689856e-08, 'reg_alpha': 0.41414596975260004, 'reg_lambda': 0.007723759913543292}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:13,493] Trial 46 finished with value: 0.6920954519826089 and parameters: {'learning_rate': 0.04277859926638299, 'min_split_gain': 8.001263706476603e-05, 'reg_alpha': 1.233608062011211e-05, 'reg_lambda': 0.0020757925227954804}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001591 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:14,385] Trial 47 finished with value: 0.7019326065035011 and parameters: {'learning_rate': 0.3139386471587729, 'min_split_gain': 0.2584180075238009, 'reg_alpha': 0.003650972981756558, 'reg_lambda': 1.345945881030354e-07}. Best is trial 36 with value: 0.7100064700948808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:15,772] Trial 48 finished with value: 0.6907546136482782 and parameters: {'learning_rate': 0.41203610932437523, 'min_split_gain': 1.768406041376781e-07, 'reg_alpha': 0.0698373856403354, 'reg_lambda': 0.3319016707682897}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:17,209] Trial 49 finished with value: 0.6982015089484411 and parameters: {'learning_rate': 0.06272951946263708, 'min_split_gain': 1.09972457070477e-08, 'reg_alpha': 0.0007705146162356091, 'reg_lambda': 6.255299373334504e-05}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001508 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:18,661] Trial 50 finished with value: 0.6656264740292045 and parameters: {'learning_rate': 0.010486942418424395, 'min_split_gain': 4.908097693882774e-07, 'reg_alpha': 1.0704013640308371e-06, 'reg_lambda': 0.014563079802114486}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001548 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:20,078] Trial 51 finished with value: 0.7025084141881549 and parameters: {'learning_rate': 0.1229948317464141, 'min_split_gain': 3.716270329883105e-08, 'reg_alpha': 0.38471530616115035, 'reg_lambda': 0.009010854101904374}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:21,491] Trial 52 finished with value: 0.7014377812504674 and parameters: {'learning_rate': 0.1426940314861754, 'min_split_gain': 2.0094049273856525e-08, 'reg_alpha': 0.4023022233023768, 'reg_lambda': 0.0049446263361434505}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:22,878] Trial 53 finished with value: 0.6546782063357535 and parameters: {'learning_rate': 0.9687776734754368, 'min_split_gain': 4.6051573595146364e-06, 'reg_alpha': 0.1555936966150283, 'reg_lambda': 0.04990295678374825}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:24,324] Trial 54 finished with value: 0.6880177000318893 and parameters: {'learning_rate': 0.03531668122204884, 'min_split_gain': 4.2684345060102475e-08, 'reg_alpha': 0.9798564424354875, 'reg_lambda': 0.00022872976856004037}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001505 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:25,799] Trial 55 finished with value: 0.6544674348985594 and parameters: {'learning_rate': 0.0009474131994634932, 'min_split_gain': 2.1333852649112556e-07, 'reg_alpha': 0.06231345503338961, 'reg_lambda': 0.0006340447230940479}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:27,173] Trial 56 finished with value: 0.6841897589631568 and parameters: {'learning_rate': 0.47112105841716945, 'min_split_gain': 9.232541664167256e-07, 'reg_alpha': 0.022883504620064058, 'reg_lambda': 0.007925208849553108}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001551 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:28,560] Trial 57 finished with value: 0.7028081566677024 and parameters: {'learning_rate': 0.18544266402117698, 'min_split_gain': 1.998397434567565e-08, 'reg_alpha': 5.6717285677441934e-05, 'reg_lambda': 0.0025172605274408873}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:29,996] Trial 58 finished with value: 0.7000877489204113 and parameters: {'learning_rate': 0.06657267570586882, 'min_split_gain': 2.038071535128986e-05, 'reg_alpha': 1.22297488597383e-05, 'reg_lambda': 1.6220110521914256e-05}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:31,439] Trial 59 finished with value: 0.6628832669116661 and parameters: {'learning_rate': 0.004899942805265832, 'min_split_gain': 7.46485474122716e-08, 'reg_alpha': 0.2276380102537267, 'reg_lambda': 0.16436629258182237}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002334 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:32,912] Trial 60 finished with value: 0.6745305722490559 and parameters: {'learning_rate': 0.0196763791019812, 'min_split_gain': 3.237900546090693e-07, 'reg_alpha': 0.6230144784155773, 'reg_lambda': 0.0003416301839758084}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:34,291] Trial 61 finished with value: 0.7026798369646009 and parameters: {'learning_rate': 0.2521602850157576, 'min_split_gain': 0.005574503166119824, 'reg_alpha': 3.3064052054174204e-05, 'reg_lambda': 0.01604585124731684}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:35,700] Trial 62 finished with value: 0.7030915598265305 and parameters: {'learning_rate': 0.11107485239636539, 'min_split_gain': 0.025054045637568235, 'reg_alpha': 0.00025933750315334673, 'reg_lambda': 0.00566778667913882}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001647 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:36,867] Trial 63 finished with value: 0.7008133727879536 and parameters: {'learning_rate': 0.1113839861970599, 'min_split_gain': 0.6301457502768048, 'reg_alpha': 0.0003050763409070586, 'reg_lambda': 0.0010846634373183704}. Best is trial 36 with value: 0.7100064700948808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001960 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:38,238] Trial 64 finished with value: 0.6829922877731246 and parameters: {'learning_rate': 0.5227955728992238, 'min_split_gain': 0.03147699637828162, 'reg_alpha': 0.005579411864226106, 'reg_lambda': 0.005032076493976}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:39,698] Trial 65 finished with value: 0.6993116909346675 and parameters: {'learning_rate': 0.06157672085688768, 'min_split_gain': 0.09021378492814468, 'reg_alpha': 0.001379924160959383, 'reg_lambda': 0.06849977098325669}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001576 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:41,144] Trial 66 finished with value: 0.68146890180925 and parameters: {'learning_rate': 0.02561146039411568, 'min_split_gain': 0.18128914805041307, 'reg_alpha': 0.10453267982758427, 'reg_lambda': 0.0026346813752179082}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:42,490] Trial 67 finished with value: 0.698752354763594 and parameters: {'learning_rate': 0.27734473195794673, 'min_split_gain': 0.014086873295822085, 'reg_alpha': 0.045002517690106915, 'reg_lambda': 0.02615231132546984}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:43,891] Trial 68 finished with value: 0.7048379631206575 and parameters: {'learning_rate': 0.12487113264270128, 'min_split_gain': 0.0019846152714903705, 'reg_alpha': 0.02372876252111761, 'reg_lambda': 0.00044423514862112167}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001549 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:45,264] Trial 69 finished with value: 0.6845055848604239 and parameters: {'learning_rate': 0.5461421927952178, 'min_split_gain': 0.001072890313084098, 'reg_alpha': 0.02101654399006945, 'reg_lambda': 0.0001578611753424715}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:46,829] Trial 70 finished with value: 0.6935588064740535 and parameters: {'learning_rate': 0.04258861484845854, 'min_split_gain': 0.00019922174207360127, 'reg_alpha': 0.21263718952934108, 'reg_lambda': 0.00041644745272847636}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:48,327] Trial 71 finished with value: 0.7040736365853748 and parameters: {'learning_rate': 0.11311927211700724, 'min_split_gain': 0.015369793036468285, 'reg_alpha': 0.015453245143748198, 'reg_lambda': 0.0053708150557071925}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:49,687] Trial 72 finished with value: 0.701506827908791 and parameters: {'learning_rate': 0.20077872243139877, 'min_split_gain': 0.00256199061830168, 'reg_alpha': 0.014990291353011678, 'reg_lambda': 0.0008803906257776599}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002686 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:51,237] Trial 73 finished with value: 0.6647563865840547 and parameters: {'learning_rate': 0.010570905661331514, 'min_split_gain': 0.041403955164536974, 'reg_alpha': 0.007036952968100711, 'reg_lambda': 0.0015244709795601127}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:52,679] Trial 74 finished with value: 0.702042298646501 and parameters: {'learning_rate': 0.08926434180561484, 'min_split_gain': 0.01482964142184269, 'reg_alpha': 0.09866905607832052, 'reg_lambda': 7.395748610569888e-05}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:54,404] Trial 75 finished with value: 0.6560214789309614 and parameters: {'learning_rate': 0.00011329659813592753, 'min_split_gain': 1.21805742137846e-07, 'reg_alpha': 0.03470337500865005, 'reg_lambda': 0.00977079612633463}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001803 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:55,873] Trial 76 finished with value: 0.6564745982331215 and parameters: {'learning_rate': 5.308131876865576e-07, 'min_split_gain': 0.011525351476940594, 'reg_alpha': 0.0033002628634132415, 'reg_lambda': 0.0005698828026156579}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001582 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003520 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:57,429] Trial 77 finished with value: 0.6957231933037478 and parameters: {'learning_rate': 0.049074522012783034, 'min_split_gain': 0.0006006412956224137, 'reg_alpha': 0.641675219755118, 'reg_lambda': 0.0033825930573404633}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:44:58,903] Trial 78 finished with value: 0.6725935724113006 and parameters: {'learning_rate': 0.6648329701309004, 'min_split_gain': 3.6675940914635645e-05, 'reg_alpha': 0.04077161286180245, 'reg_lambda': 0.00020106599119750358}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:00,304] Trial 79 finished with value: 0.6984390288872815 and parameters: {'learning_rate': 0.280467469398077, 'min_split_gain': 1.0711297662715009e-08, 'reg_alpha': 0.47665185347585226, 'reg_lambda': 2.1623568824185473e-05}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:01,768] Trial 80 finished with value: 0.6814510461040588 and parameters: {'learning_rate': 0.02446062798877687, 'min_split_gain': 6.23313472685586e-08, 'reg_alpha': 0.018759555772561227, 'reg_lambda': 0.00159263229265743}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001620 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:03,184] Trial 81 finished with value: 0.7025628383564876 and parameters: {'learning_rate': 0.1230679953052827, 'min_split_gain': 0.007901739897139609, 'reg_alpha': 3.079047490204278e-06, 'reg_lambda': 0.004306514627503824}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:04,641] Trial 82 finished with value: 0.7009449631389087 and parameters: {'learning_rate': 0.09244442487964978, 'min_split_gain': 0.0035136054816136066, 'reg_alpha': 0.00015094246534377433, 'reg_lambda': 0.007215129581902444}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:06,051] Trial 83 finished with value: 0.7030463628911355 and parameters: {'learning_rate': 0.16717157488463813, 'min_split_gain': 0.022928611233562785, 'reg_alpha': 0.008758764186476033, 'reg_lambda': 0.013228822243864747}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:07,514] Trial 84 finished with value: 0.6870780999063172 and parameters: {'learning_rate': 0.03363758515500325, 'min_split_gain': 0.04317940441678239, 'reg_alpha': 1.3384616465211534e-05, 'reg_lambda': 0.0009163636433810924}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:08,972] Trial 85 finished with value: 0.6699399023774951 and parameters: {'learning_rate': 0.01547293154819163, 'min_split_gain': 3.152456018703914e-08, 'reg_alpha': 0.08174397900832181, 'reg_lambda': 0.0003233836014001213}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:10,070] Trial 86 finished with value: 0.7010900034825708 and parameters: {'learning_rate': 0.33265998738585195, 'min_split_gain': 0.10993075435226922, 'reg_alpha': 0.2489517125093687, 'reg_lambda': 0.02197268431956705}. Best is trial 36 with value: 0.7100064700948808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:11,480] Trial 87 finished with value: 0.7035015147619686 and parameters: {'learning_rate': 0.08864948203383721, 'min_split_gain': 0.06950719642655541, 'reg_alpha': 0.17416236639050528, 'reg_lambda': 0.0024660653700098946}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002603 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001512 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:12,217] Trial 88 finished with value: 0.6718676815562675 and parameters: {'learning_rate': 0.9977300878877974, 'min_split_gain': 0.2189765105015317, 'reg_alpha': 0.161641221630265, 'reg_lambda': 0.03780545806451016}. Best is trial 36 with value: 0.7100064700948808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001810 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:13,652] Trial 89 finished with value: 0.6989393733152994 and parameters: {'learning_rate': 0.06560657069789161, 'min_split_gain': 0.4407800525141084, 'reg_alpha': 0.03095693300649155, 'reg_lambda': 0.002210436734872558}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:15,101] Trial 90 finished with value: 0.6551272410564736 and parameters: {'learning_rate': 2.270711424146332e-05, 'min_split_gain': 0.0001666301332278944, 'reg_alpha': 0.1808509918208121, 'reg_lambda': 0.00013071122284035887}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:16,601] Trial 91 finished with value: 0.7016750203212981 and parameters: {'learning_rate': 0.14015158200951486, 'min_split_gain': 0.058367572376207814, 'reg_alpha': 0.31317765061907776, 'reg_lambda': 0.005576221671936801}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001679 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:18,026] Trial 92 finished with value: 0.7056332091704743 and parameters: {'learning_rate': 0.19856065030282988, 'min_split_gain': 0.009250953818602933, 'reg_alpha': 0.05096060186045952, 'reg_lambda': 0.0011454115941637241}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:19,435] Trial 93 finished with value: 0.6936723591625894 and parameters: {'learning_rate': 0.36357145092614035, 'min_split_gain': 1.569526686628604e-08, 'reg_alpha': 0.05053285714152832, 'reg_lambda': 0.0005521221156642845}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:20,879] Trial 94 finished with value: 0.7017758821780653 and parameters: {'learning_rate': 0.07783160370807832, 'min_split_gain': 7.932982313948368e-06, 'reg_alpha': 0.1037888521798141, 'reg_lambda': 0.0012013142553325376}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001544 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:22,273] Trial 95 finished with value: 0.7017308052140638 and parameters: {'learning_rate': 0.2048292150623995, 'min_split_gain': 0.1560851477525166, 'reg_alpha': 0.005010321946789194, 'reg_lambda': 0.0030416170854016696}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:23,683] Trial 96 finished with value: 0.6771748111231611 and parameters: {'learning_rate': 0.6022801823530053, 'min_split_gain': 0.010014757271544513, 'reg_alpha': 0.013148608941164546, 'reg_lambda': 5.5196584354792546e-05}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001552 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:25,094] Trial 97 finished with value: 0.7032532257031101 and parameters: {'learning_rate': 0.21194747450953824, 'min_split_gain': 4.820313184111436e-08, 'reg_alpha': 0.5629522961686743, 'reg_lambda': 0.0007059522171261836}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:26,587] Trial 98 finished with value: 0.6919476598900057 and parameters: {'learning_rate': 0.0439085628257644, 'min_split_gain': 0.0023504717172266164, 'reg_alpha': 0.05912641088596169, 'reg_lambda': 0.00027941889976367196}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 674\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 80\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8527\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 664\n",
      "[LightGBM] [Info] Number of data points in the train set: 10853, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2326, number of negative: 8528\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 675\n",
      "[LightGBM] [Info] Number of data points in the train set: 10854, number of used features: 81\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:27,997] Trial 99 finished with value: 0.7031056866631631 and parameters: {'learning_rate': 0.153900360918319, 'min_split_gain': 0.017505282289823784, 'reg_alpha': 0.031100706989262913, 'reg_lambda': 0.0017828966750929609}. Best is trial 36 with value: 0.7100064700948808.\n",
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4352, number of negative: 11929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 744\n",
      "[LightGBM] [Info] Number of data points in the train set: 16281, number of used features: 87\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.267305 -> initscore=-1.008337\n",
      "[LightGBM] [Info] Start training from score -1.008337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "[I 2025-02-04 19:45:28,523] A new study created in memory with name: no-name-79edd29e-0eaa-4301-9a58-70ec3493eb84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-04 19:50:35,495] Trial 0 finished with value: -0.8458998771498771 and parameters: {'lambda_reg': 0.09508069760825165}. Best is trial 0 with value: -0.8458998771498771.\n",
      "[I 2025-02-04 19:55:12,070] Trial 1 finished with value: -0.8467444717444718 and parameters: {'lambda_reg': 0.284301540813457}. Best is trial 1 with value: -0.8467444717444718.\n",
      "[I 2025-02-04 19:59:56,879] Trial 2 finished with value: -0.8441339066339066 and parameters: {'lambda_reg': 0.48780444915394205}. Best is trial 1 with value: -0.8467444717444718.\n",
      "[I 2025-02-04 20:05:04,026] Trial 3 finished with value: -0.8466676904176904 and parameters: {'lambda_reg': 0.15095587273981192}. Best is trial 1 with value: -0.8467444717444718.\n",
      "[I 2025-02-04 20:09:51,033] Trial 4 finished with value: -0.8457463144963145 and parameters: {'lambda_reg': 0.22917617119891798}. Best is trial 1 with value: -0.8467444717444718.\n",
      "[I 2025-02-04 20:14:56,372] Trial 5 finished with value: -0.8468980343980343 and parameters: {'lambda_reg': 0.07392572839885683}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:19:52,098] Trial 6 finished with value: -0.8447481572481572 and parameters: {'lambda_reg': 0.46560830363335565}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:25:22,778] Trial 7 finished with value: -0.8458998771498771 and parameters: {'lambda_reg': 0.09403350906289232}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:30:22,293] Trial 8 finished with value: -0.8467444717444718 and parameters: {'lambda_reg': 0.0629328330597576}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:35:02,387] Trial 9 finished with value: -0.8465909090909091 and parameters: {'lambda_reg': 0.3070016750878217}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:39:53,328] Trial 10 finished with value: -0.8462070024570024 and parameters: {'lambda_reg': 0.0547677339967465}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:44:45,164] Trial 11 finished with value: -0.8460534398034398 and parameters: {'lambda_reg': 0.1773458719689052}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:49:24,282] Trial 12 finished with value: -0.8468212530712531 and parameters: {'lambda_reg': 0.2845199364181796}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:54:36,157] Trial 13 finished with value: -0.8458230958230958 and parameters: {'lambda_reg': 0.09663799729659014}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 20:59:50,364] Trial 14 finished with value: -0.8466676904176904 and parameters: {'lambda_reg': 0.14298123196794432}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 21:04:26,773] Trial 15 finished with value: -0.8462837837837838 and parameters: {'lambda_reg': 0.342218550200831}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 21:09:09,953] Trial 16 finished with value: -0.8452856265356266 and parameters: {'lambda_reg': 0.20327882122852617}. Best is trial 5 with value: -0.8468980343980343.\n",
      "[I 2025-02-04 21:14:23,804] Trial 17 finished with value: -0.8465141277641277 and parameters: {'lambda_reg': 0.12244292553471736}. Best is trial 5 with value: -0.8468980343980343.\n"
     ]
    }
   ],
   "source": [
    "datasets = ['Adult', 'Bank']\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nProcessing {dataset} dataset\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    if dataset == '7-point':\n",
    "        X_old, y_old, X_new, y_new, X_clinical = load_prep_data(dataset)\n",
    "    else:\n",
    "        X_old, y_old, X_new, y_new, feature_names, x = load_prep_data(dataset)\n",
    "    \n",
    "    X_new, X_old = np.array(X_new), np.array(X_old)\n",
    "    y_new, y_old = np.array(y_new), np.array(y_old)\n",
    "    \n",
    "    # 2. Train base models on old data\n",
    "    #base_models = ['L1-LR', 'L2-LR', 'DT', 'RF', 'LGBM']\n",
    "    base_models = ['DT', 'RF', 'LGBM']\n",
    "    base_models_dict = train_old_model(dataset, X_new, X_old, y_new, y_old)\n",
    "    \n",
    "    # 3. Initialize metrics dictionaries for both base and corrected models\n",
    "    metrics = {\n",
    "        'Accuracy': {model: [] for model in base_models + [f'{m}_LCT' for m in base_models]},\n",
    "        'Precision': {model: [] for model in base_models + [f'{m}_LCT' for m in base_models]},\n",
    "        'Recall': {model: [] for model in base_models + [f'{m}_LCT' for m in base_models]},\n",
    "        'F1': {model: [] for model in base_models + [f'{m}_LCT' for m in base_models]}\n",
    "    }\n",
    "    \n",
    "    # 4. Perform 5-fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_new, y_new)):\n",
    "        print(f\"Processing fold {fold + 1}/5\")\n",
    "        \n",
    "        X_train, X_test = X_new[train_idx], X_new[test_idx]\n",
    "        y_train, y_test = y_new[train_idx], y_new[test_idx]\n",
    "        \n",
    "        # Train LCT-corrected models on fold training data\n",
    "        fold_corrected = train_new_models(dataset, X_train, y_train, base_models_dict, LocalCorrectionTree)\n",
    "        \n",
    "        # Evaluate both base and corrected models\n",
    "        for model in base_models:\n",
    "            # Evaluate base model\n",
    "            if model == 'DNN':\n",
    "                base_preds = base_models_dict[model].predict(X_test)\n",
    "                final_preds_base = base_preds\n",
    "            else:\n",
    "                base_probs = base_models_dict[model].predict_proba(X_test)\n",
    "                final_preds_base = np.argmax(base_probs, axis=1)\n",
    "            \n",
    "            # Calculate base model metrics\n",
    "            metrics['Accuracy'][model].append(accuracy_score(y_test, final_preds_base))\n",
    "            metrics['Precision'][model].append(precision_score(y_test, final_preds_base, average='macro'))\n",
    "            metrics['Recall'][model].append(recall_score(y_test, final_preds_base, average='macro'))\n",
    "            metrics['F1'][model].append(f1_score(y_test, final_preds_base, average='macro'))\n",
    "            \n",
    "            # Evaluate LCT-corrected model\n",
    "            lct_model = fold_corrected[f'{model}_LCT']\n",
    "            corrections = lct_model.predict(X_test)\n",
    "            corrected_probs = base_probs + corrections\n",
    "            final_preds_lct = np.argmax(corrected_probs, axis=1)\n",
    "            \n",
    "            # Calculate LCT model metrics\n",
    "            metrics['Accuracy'][f'{model}_LCT'].append(accuracy_score(y_test, final_preds_lct))\n",
    "            metrics['Precision'][f'{model}_LCT'].append(precision_score(y_test, final_preds_lct, average='macro'))\n",
    "            metrics['Recall'][f'{model}_LCT'].append(recall_score(y_test, final_preds_lct, average='macro'))\n",
    "            metrics['F1'][f'{model}_LCT'].append(f1_score(y_test, final_preds_lct, average='macro'))\n",
    "    \n",
    "    # 5. Prepare metrics for comparison plot\n",
    "    plot_metrics = {\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': []\n",
    "    }\n",
    "    \n",
    "    all_models = base_models + [f'{m}_LCT' for m in base_models]\n",
    "    for metric in plot_metrics:\n",
    "        for model in all_models:\n",
    "            plot_metrics[metric].append(metrics[metric][model])\n",
    "    \n",
    "    # 6. Create and save comparison plot\n",
    "    fig = plot_metrics_comparison(dataset, plot_metrics, all_models)\n",
    "    plt.savefig(f'{dataset}_combined_comparison.png', bbox_inches='tight', dpi=400)\n",
    "    plt.close()\n",
    "    \n",
    "    # 7. Print numerical results\n",
    "    print(f\"\\nCombined Model Results for {dataset}:\")\n",
    "    for metric in plot_metrics:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for name in all_models:\n",
    "            mean_val = np.mean(metrics[metric][name])\n",
    "            std_val = np.std(metrics[metric][name])\n",
    "            print(f\"{name}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "print(\"\\nDone evaluating models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''datasets = ['Adult']\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nProcessing {dataset} dataset\")\n",
    "    \n",
    "    # Define base and enhanced models\n",
    "    base_models = ['L1-LR']# 'L2-LR', 'DT', 'RF', 'LGBM']\n",
    "    enhanced_models = ['L1-LR+']#, 'L2-LR+', 'DT+', 'Ours']\n",
    "                       #, 'RF+', 'LGBM+', 'LGBM+C', 'Ours']\n",
    "    all_models = base_models + enhanced_models\n",
    "    \n",
    "    # 1. Load data\n",
    "    if dataset == '7-point':\n",
    "        X_old, y_old, X_new, y_new, X_clinical = load_prep_data(dataset)\n",
    "    else:\n",
    "        X_old, y_old, X_new, y_new, feature_names, x = load_prep_data(dataset)\n",
    "    \n",
    "    X_new, X_old = np.array(X_new[:100]), np.array(X_old[:100])\n",
    "    y_new, y_old = np.array(y_new[:100]), np.array(y_old[:100])\n",
    "    \n",
    "    # 2. Train base models on old data\n",
    "    base_models_dict = train_old_model(dataset, X_new, X_old, y_new, y_old)\n",
    "    \n",
    "    # 3. Train enhanced models on new data\n",
    "    enhanced_models_dict = train_new_models(dataset, X_new, y_new, base_models_dict, LocalCorrectionTree)\n",
    "\n",
    "    # 4. Perform 5-fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    all_metrics = {\n",
    "        'Accuracy': {model: [] for model in base_models + list(enhanced_models_dict.keys())},\n",
    "        'Precision': {model: [] for model in base_models + list(enhanced_models_dict.keys())},\n",
    "        'Recall': {model: [] for model in base_models + list(enhanced_models_dict.keys())},\n",
    "        'F1': {model: [] for model in base_models + list(enhanced_models_dict.keys())}\n",
    "    }\n",
    "        \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_new, y_new)):\n",
    "        print(f\"Processing fold {fold + 1}/5\")\n",
    "    \n",
    "        X_train, X_test = X_new[train_idx], X_new[test_idx]\n",
    "        y_train, y_test = y_new[train_idx], y_new[test_idx]\n",
    "        \n",
    "        # Evaluate each base model individually\n",
    "        for model_name in base_models:\n",
    "            fold_metrics = evaluate_old_model(dataset, X_test, y_test, base_models_dict[model_name])\n",
    "            \n",
    "            for metric in all_metrics:\n",
    "                all_metrics[metric][model_name].extend(fold_metrics[metric])\n",
    "        \n",
    "        # Evaluate each enhanced model individually\n",
    "        for model_name, model in enhanced_models_dict.items():\n",
    "            old_model_scores = base_models_dict[model[:-4]].predict_proba(X_train)\n",
    "            model.fit(X_train, y_train, old_model_scores)\n",
    "            fold_metrics = evaluate_new_models(dataset, {model_name: model}, X_test, y_test, base_models_dict)\n",
    "            \n",
    "            for metric in all_metrics:\n",
    "                all_metrics[metric][model_name].extend(fold_metrics[metric])\n",
    "\n",
    "# 5. Prepare metrics for plotting\n",
    "plot_metrics = {\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': []\n",
    "}\n",
    "\n",
    "for metric in plot_metrics:\n",
    "    for model in base_models + list(enhanced_models_dict.keys()):\n",
    "        plot_metrics[metric].append(all_metrics[metric][model])\n",
    "\n",
    "# 6. Create and save plot\n",
    "fig = plot_metrics_comparison(dataset, plot_metrics, base_models + list(enhanced_models_dict.keys()))\n",
    "plt.savefig(f'{dataset}_all_models_comparison.png', bbox_inches='tight', dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# 7. Print numerical results\n",
    "print(f\"\\nAll Model Results for {dataset}:\")\n",
    "for metric in plot_metrics:\n",
    "    print(f\"\\n{metric}:\")\n",
    "    for name, values in zip(base_models + list(enhanced_models_dict.keys()), plot_metrics[metric]):\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        print(f\"{name}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "# 8. Print final Local Correction Tree structure\n",
    "for model_name, model in enhanced_models_dict.items():\n",
    "    if model_name.endswith('+'):\n",
    "        print(\"\\nFinal Local Correction Tree Structure:\")\n",
    "        model.simplify()\n",
    "        print_tree(model, feature_names=feature_names)\n",
    "\n",
    "print(\"\\nDone evaluating all models.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''datasets = ['Adult', 'Bank']\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nProcessing {dataset} dataset\")\n",
    "    \n",
    "    # Define only base models\n",
    "    base_models = ['L1-LR', 'L2-LR', 'DT', 'RF', 'LGBM']\n",
    "    \n",
    "    # 1. Load data\n",
    "    if dataset == '7-point':\n",
    "        X_old, y_old, X_new, y_new, X_clinical = load_prep_data(dataset)\n",
    "    else:\n",
    "        X_old, y_old, X_new, y_new, feature_names, x = load_prep_data(dataset)\n",
    "    \n",
    "    X_new, X_old = np.array(X_new), np.array(X_old)\n",
    "    y_new, y_old = np.array(y_new), np.array(y_old)\n",
    "    \n",
    "    # 2. Train base models on old data\n",
    "    base_models_dict = train_old_model(dataset, X_new, X_old, y_new, y_old)\n",
    "    \n",
    "    # 3. Perform 5-fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    all_metrics = {\n",
    "        'Accuracy': {model: [] for model in base_models},\n",
    "        'Precision': {model: [] for model in base_models},\n",
    "        'Recall': {model: [] for model in base_models},\n",
    "        'F1': {model: [] for model in base_models}\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_old, y_old)):\n",
    "        print(f\"Processing fold {fold + 1}/5\")\n",
    "        \n",
    "        X_train, X_test = X_old[train_idx], X_old[test_idx]\n",
    "        y_train, y_test = y_old[train_idx], y_old[test_idx]\n",
    "        \n",
    "        # Evaluate each base model individually\n",
    "        for model_name in base_models:\n",
    "            fold_metrics = evaluate_old_model(dataset, X_test, y_test, base_models_dict[model_name])\n",
    "            \n",
    "            for metric in all_metrics:\n",
    "                all_metrics[metric][model_name].extend(fold_metrics[metric])\n",
    "    \n",
    "    # 4. Prepare metrics for plotting\n",
    "    plot_metrics = {\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': []\n",
    "    }\n",
    "    \n",
    "    for metric in plot_metrics:\n",
    "        for model in base_models:\n",
    "            plot_metrics[metric].append(all_metrics[metric][model])\n",
    "    \n",
    "    # 5. Create and save plot\n",
    "    fig = plot_metrics_comparison(dataset, plot_metrics, base_models)\n",
    "    plt.savefig(f'{dataset}_base_models_comparison.png', bbox_inches='tight', dpi=400)\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Print numerical results\n",
    "    print(f\"\\nBase Model Results for {dataset}:\")\n",
    "    for metric in plot_metrics:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for name, values in zip(base_models, plot_metrics[metric]):\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            print(f\"{name}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "\n",
    "print(\"\\nDone evaluating base models.\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_correction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
