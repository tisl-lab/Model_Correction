{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.data import AdultDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name in ['Adult', 'Bank']:\n",
    "        dataset = AdultDataset()\n",
    "        X, y, feature_names, x = dataset.load_data()\n",
    "        X_old, y_old, X_new, y_new = dataset.split_old_new_data(X, y)\n",
    "        return X_old, y_old, X_new, y_new, feature_names, x\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_old, y_old, X_new, y_new, feature_names, x = load_data('Adult')\n",
    "# Verify the split\n",
    "print(f\"Old data shape: {X_old}, Positive samples: {y_old.sum()}\")\n",
    "print(f\"New data shape: {X_new}, Positive samples: {y_new.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.models import train_old_model, train_new_models\n",
    "from Models.lct import LocalCorrectionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_old_model(dataset, X, y, old_model):\n",
    "    \"\"\"\n",
    "    Evaluate the old model on the entire new dataset (X, y).\n",
    "    Returns a dictionary of metrics with single values.\n",
    "    \"\"\"\n",
    "    y_pred = old_model.predict(X)\n",
    "    is_multiclass = dataset in ['CTG', '7-point']\n",
    "    \n",
    "    if is_multiclass:\n",
    "        precision = precision_score(y, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y, y_pred, average='macro', zero_division=0)\n",
    "    else:\n",
    "        precision = precision_score(y, y_pred, zero_division=0)\n",
    "        recall = recall_score(y, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y, y_pred, zero_division=0)\n",
    "    \n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': [accuracy],\n",
    "        'Precision': [precision],\n",
    "        'Recall': [recall],\n",
    "        'F1': [f1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_new_models(dataset, models, X_test, y_test, old_model):\n",
    "    \"\"\"\n",
    "    Evaluate new models including LCT on the test set.\n",
    "    Returns metrics formatted for error bar plotting.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': []\n",
    "    }\n",
    "    \n",
    "    is_multiclass = dataset in ['CTG', '7-point']\n",
    "    old_scores_test = old_model.predict_proba(X_test)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        if model_name == 'Ours':  # LCT model\n",
    "            corrections = model.predict(X_test)\n",
    "            corrected_scores = old_scores_test + corrections\n",
    "            y_pred = np.argmax(corrected_scores, axis=1)\n",
    "        else:\n",
    "            if model_name.endswith('+'):\n",
    "                # Models with old scores as additional features\n",
    "                X_test_with_scores = np.hstack([X_test, old_scores_test])\n",
    "                y_pred = model.predict(X_test_with_scores)\n",
    "            else:\n",
    "                # Base models\n",
    "                y_pred = model.predict(X_test)\n",
    "        \n",
    "        if is_multiclass:\n",
    "            precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        metrics['Accuracy'].append(accuracy)\n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1'].append(f1)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(dataset, metrics, model_names):\n",
    "    \"\"\"\n",
    "    Create comparison plots with error bars for model metrics.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    \n",
    "    # Define model colors and markers\n",
    "    colors = {\n",
    "        'L1-LR': '#1f77b4',\n",
    "        'L1-LR+': '#1f77b4',\n",
    "        'L2-LR': '#ff7f0e',\n",
    "        'L2-LR+': '#ff7f0e',\n",
    "        'DT': '#2ca02c',\n",
    "        'DT+': '#2ca02c',\n",
    "        'RF': '#8c564b',\n",
    "        'RF+': '#8c564b',\n",
    "        'LGBM': '#9467bd',\n",
    "        'LGBM+': '#9467bd',\n",
    "        'LGBM+C': '#9467bd',\n",
    "        'Ours': '#d62728',\n",
    "        'Old': '#7f7f7f'\n",
    "    }\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_names):\n",
    "        ax = axes[idx]\n",
    "        y_pos = np.arange(len(model_names))\n",
    "        values = metrics[metric]\n",
    "        \n",
    "        # Plot horizontal lines with error bars\n",
    "        for i, (name, value) in enumerate(zip(model_names, values)):\n",
    "            color = colors.get(name, 'black')\n",
    "            mean_val = np.mean(value)\n",
    "            std_val = np.std(value)\n",
    "            \n",
    "            # Plot horizontal line\n",
    "            line_length = 0.02\n",
    "            ax.plot([mean_val-line_length, mean_val+line_length], [i, i],\n",
    "                   color=color, linewidth=2, solid_capstyle='butt')\n",
    "            \n",
    "            # Plot error bars\n",
    "            if name.endswith('+'):\n",
    "                marker = 'o'\n",
    "                fillstyle = 'none'\n",
    "            else:\n",
    "                marker = 'o'\n",
    "                fillstyle = 'full'\n",
    "            \n",
    "            ax.errorbar(mean_val, i, xerr=std_val,\n",
    "                       fmt=marker, color=color, capsize=3,\n",
    "                       markersize=4, fillstyle=fillstyle,\n",
    "                       elinewidth=1, capthick=1)\n",
    "        \n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(model_names)\n",
    "        ax.set_title(f'({chr(97+idx)}) {metric} ({dataset})')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Set x-axis limits and ticks\n",
    "        all_values = [np.mean(v) for v in values]\n",
    "        min_val = min(all_values) - 0.05\n",
    "        max_val = max(all_values) + 0.05\n",
    "        ax.set_xlim(min_val, max_val)\n",
    "        \n",
    "        # Format x-axis ticks to match the image\n",
    "        ax.set_xticks(np.linspace(min_val, max_val, 5))\n",
    "        ax.set_xticklabels([f'{x:.2f}' for x in ax.get_xticks()])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(lct, node_id=0, depth=0, feature_names=None):\n",
    "    \"\"\"\n",
    "    Print the Local Correction Tree structure.\n",
    "    Args:\n",
    "        lct: LocalCorrectionTree instance\n",
    "        node_id: Current node ID\n",
    "        depth: Current depth in tree\n",
    "        feature_names: Optional list of feature names\n",
    "    \"\"\"\n",
    "    feature_idx, threshold, w_node = lct.nodes[node_id]\n",
    "    left_id, right_id = lct.children[node_id]\n",
    "    \n",
    "    indent = \"  \" * depth\n",
    "    if feature_idx == -1:\n",
    "        print(f\"{indent}Leaf: correction = {w_node.round(4)}\")\n",
    "    else:\n",
    "        feature = f\"X[{feature_idx}]\" if feature_names is None else feature_names[feature_idx]\n",
    "        print(f\"{indent}Node: {feature} <= {threshold:.4f}\")\n",
    "        print_tree(lct, left_id, depth + 1, feature_names)\n",
    "        print_tree(lct, right_id, depth + 1, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified main execution\n",
    "datasets = ['Adult']\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nProcessing {dataset} dataset\")\n",
    "    \n",
    "    base_models = ['L1-LR', 'L2-LR', 'DT', 'RF', 'LGBM']\n",
    "    enhanced_models = ['L1-LR+', 'L2-LR+', 'DT+', 'RF+', 'LGBM+', 'LGBM+C', 'Ours']\n",
    "    model_names = base_models + enhanced_models + ['Old']\n",
    "    \n",
    "    # 1. Load data\n",
    "    if dataset == '7-point':\n",
    "        X_old, y_old, X_new, y_new, X_clinical = load_data(dataset)\n",
    "    else:\n",
    "        X_old, y_old, X_new, y_new, feature_names, x = load_data(dataset)\n",
    "    \n",
    "    X_new = np.array(X_new)\n",
    "    y_new = np.array(y_new)\n",
    "    \n",
    "    # 2. Train and evaluate old model with multiple runs\n",
    "    n_runs = 5\n",
    "    old_metrics_runs = {\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': []\n",
    "    }\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        old_model = train_old_model(dataset, X_old, y_old)\n",
    "        run_metrics = evaluate_old_model(dataset, X_new, y_new, old_model)\n",
    "        for metric in old_metrics_runs:\n",
    "            old_metrics_runs[metric].append(run_metrics[metric][0])\n",
    "    \n",
    "    # 3. Perform 5-fold cross validation for new models\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    all_metrics = {\n",
    "        'Accuracy': {model: [] for model in model_names[:-1]},\n",
    "        'Precision': {model: [] for model in model_names[:-1]},\n",
    "        'Recall': {model: [] for model in model_names[:-1]},\n",
    "        'F1': {model: [] for model in model_names[:-1]}\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X_new, y_new)):\n",
    "        print(f\"Processing fold {fold + 1}/5\")\n",
    "        \n",
    "        X_train, X_test = X_new[train_idx], X_new[test_idx]\n",
    "        y_train, y_test = y_new[train_idx], y_new[test_idx]\n",
    "        \n",
    "        # Get old model predictions for enhanced models\n",
    "        old_scores_train = old_model.predict_proba(X_train)\n",
    "        old_scores_test = old_model.predict_proba(X_test)\n",
    "        \n",
    "        # Train base models\n",
    "        base_model_dict = train_new_models(dataset, X_train, y_train, old_model, LocalCorrectionTree)\n",
    "        \n",
    "        # Train enhanced models with old model scores\n",
    "        X_train_enhanced = np.hstack([X_train, old_scores_train])\n",
    "        X_test_enhanced = np.hstack([X_test, old_scores_test])\n",
    "        enhanced_model_dict = train_new_models(dataset, X_train_enhanced, y_train, old_model, LocalCorrectionTree)\n",
    "        \n",
    "        # Combine all models\n",
    "        fold_models = {**base_model_dict, **enhanced_model_dict}\n",
    "        \n",
    "        # Evaluate models\n",
    "        fold_metrics = evaluate_new_models(dataset, fold_models, X_test, y_test, old_model)\n",
    "        \n",
    "        for metric in all_metrics:\n",
    "            for model in model_names[:-1]:\n",
    "                if model in base_models:\n",
    "                    all_metrics[metric][model].append(fold_metrics[metric][model])\n",
    "                else:\n",
    "                    # For enhanced models, use enhanced data\n",
    "                    if model == 'Ours':\n",
    "                        # LCT uses original features but adds corrections\n",
    "                        all_metrics[metric][model].append(fold_metrics[metric][model])\n",
    "                    else:\n",
    "                        all_metrics[metric][model].append(fold_metrics[metric][model])\n",
    "                        \n",
    "    print(\"\\nFinal Local Correction Tree Structure:\")\n",
    "    if 'Ours' in enhanced_models:  # Check if LCT model exists\n",
    "        enhanced_models['Ours'].simplify()  # Remove redundant nodes\n",
    "        print_tree(enhanced_models['Ours'], feature_names=feature_names)\n",
    "    \n",
    "    # 4. Prepare metrics for plotting\n",
    "    plot_metrics = {\n",
    "        'Accuracy': [],\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': []\n",
    "    }\n",
    "    \n",
    "    for metric in plot_metrics:\n",
    "        plot_metrics[metric].append(old_metrics_runs[metric])\n",
    "        for model in model_names[:-1]:\n",
    "            plot_metrics[metric].append(all_metrics[metric][model])\n",
    "    \n",
    "    # 5. Create and save plot\n",
    "    fig = plot_metrics_comparison(dataset, plot_metrics, model_names)\n",
    "    plt.savefig(f'{dataset}_model_comparison.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Print numerical results\n",
    "    print(f\"\\nResults for {dataset}:\")\n",
    "    for metric in plot_metrics:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        for name, values in zip(model_names, plot_metrics[metric]):\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            print(f\"{name}: {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "\n",
    "print(\"\\nDone evaluating old model and new models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_correction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
