{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davembiazi/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna.logging\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.model import train_lct, train_old_model, train_base_competitor_models,  train_competitor_standard_corrected\n",
    "from Models.lct import LocalCorrectionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "def load_prep_data(dataset_name):\n",
    "    if dataset_name in ['Adult', 'Bank']:\n",
    "        dataset = Dataset(dataset_name)\n",
    "        X, y, feature_names, x = dataset.load_data()\n",
    "        X_old, y_old, X_new, y_new = dataset.split_old_new_data(X, y)\n",
    "        return X_old, y_old, X_new, y_new, feature_names, x\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom lightgbm import LGBMClassifier as LGBM\\n\\ndataset = Dataset(\"Bank\")\\nX, y, feature_names, raw_data = dataset.load_data()\\nX, y = np.array(X), np.array(y)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n\\nold_model = LGBM(learning_rate=0.2, min_split_gain=0.3, reg_alpha=0.2, \\n                 reg_lambda=0.2, class_weight=\\'balanced\\')\\nold_model.fit(X_train, y_train)\\n\\n# Get old model scores on the test set\\nold_scores = old_model.predict_proba(X_test)\\n\\n# Instantiate and fit the Local Correction Tree\\nlct = LocalCorrectionTree()\\nlct.fit(X_test, y_test, old_scores)\\n\\n#lct.prune(X_test, y_test, old_scores)\\n#lct.simplify()\\n\\n# Get corrections from LCT\\ncorrections = lct.predict(X_test)\\n\\n# Apply corrections to old model scores\\nnew_scores = old_scores + corrections\\n\\n# Analyze the corrections\\nnon_zero_corrections = np.sum(np.any(corrections != 0, axis=1))\\nprint(f\"Number of samples with non-zero corrections: {non_zero_corrections}\")\\nprint(f\"Percentage of samples corrected: {non_zero_corrections / len(X_test) * 100:.2f}%\")\\n\\n # Evaluate overall performance.\\nold_preds = np.argmax(old_scores, axis=1)\\nnew_preds = np.argmax(new_scores, axis=1)\\nold_accuracy = np.mean(old_preds == y_test)\\nnew_accuracy = np.mean(new_preds == y_test)\\nnon_zero_corr = np.sum(np.linalg.norm(corrections, axis=1) > 1e-8)\\n\\nprint(\"Old model accuracy:\", round(old_accuracy, 4))\\nprint(\"Corrected model accuracy:\", round(new_accuracy, 4))\\nprint(\"Number of samples with non-zero corrections:\", non_zero_corr)\\nprint(\"Percentage of samples corrected: {:.2f}%\".format(non_zero_corr / len(X_test) * 100))\\nprint(\"\\nExample corrections:\")\\nfor i in range(10):\\n    print(\"Sample\", i, \":\")\\n    print(\"  Old scores:\", old_scores[i])\\n    print(\"  Correction:\", corrections[i])\\n    print(\"  New scores:\", new_scores[i])\\n    print(\"  True label:\", y_test[i])\\n    print(\"  Predicted label before correction:\", old_preds[i])\\n    print(\"  Predicted label after correction:\", new_preds[i])\\n    print()\\n\\nprint(\"Simplified tree structure:\")\\nlct.print_tree(feature_names=feature_names) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier as LGBM\n",
    "\n",
    "dataset = Dataset(\"Bank\")\n",
    "X, y, feature_names, raw_data = dataset.load_data()\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "old_model = LGBM(learning_rate=0.2, min_split_gain=0.3, reg_alpha=0.2, \n",
    "                 reg_lambda=0.2, class_weight='balanced')\n",
    "old_model.fit(X_train, y_train)\n",
    "\n",
    "# Get old model scores on the test set\n",
    "old_scores = old_model.predict_proba(X_test)\n",
    "\n",
    "# Instantiate and fit the Local Correction Tree\n",
    "lct = LocalCorrectionTree()\n",
    "lct.fit(X_test, y_test, old_scores)\n",
    "\n",
    "#lct.prune(X_test, y_test, old_scores)\n",
    "#lct.simplify()\n",
    "\n",
    "# Get corrections from LCT\n",
    "corrections = lct.predict(X_test)\n",
    "\n",
    "# Apply corrections to old model scores\n",
    "new_scores = old_scores + corrections\n",
    "\n",
    "# Analyze the corrections\n",
    "non_zero_corrections = np.sum(np.any(corrections != 0, axis=1))\n",
    "print(f\"Number of samples with non-zero corrections: {non_zero_corrections}\")\n",
    "print(f\"Percentage of samples corrected: {non_zero_corrections / len(X_test) * 100:.2f}%\")\n",
    "\n",
    " # Evaluate overall performance.\n",
    "old_preds = np.argmax(old_scores, axis=1)\n",
    "new_preds = np.argmax(new_scores, axis=1)\n",
    "old_accuracy = np.mean(old_preds == y_test)\n",
    "new_accuracy = np.mean(new_preds == y_test)\n",
    "non_zero_corr = np.sum(np.linalg.norm(corrections, axis=1) > 1e-8)\n",
    "\n",
    "print(\"Old model accuracy:\", round(old_accuracy, 4))\n",
    "print(\"Corrected model accuracy:\", round(new_accuracy, 4))\n",
    "print(\"Number of samples with non-zero corrections:\", non_zero_corr)\n",
    "print(\"Percentage of samples corrected: {:.2f}%\".format(non_zero_corr / len(X_test) * 100))\n",
    "print(\"\\nExample corrections:\")\n",
    "for i in range(10):\n",
    "    print(\"Sample\", i, \":\")\n",
    "    print(\"  Old scores:\", old_scores[i])\n",
    "    print(\"  Correction:\", corrections[i])\n",
    "    print(\"  New scores:\", new_scores[i])\n",
    "    print(\"  True label:\", y_test[i])\n",
    "    print(\"  Predicted label before correction:\", old_preds[i])\n",
    "    print(\"  Predicted label after correction:\", new_preds[i])\n",
    "    print()\n",
    "\n",
    "print(\"Simplified tree structure:\")\n",
    "lct.print_tree(feature_names=feature_names) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def _find_w(self, indices, old_scores, y, max_iter=10):\\n        #n_samples = len(indices)\\n        n_classes = old_scores.shape[1]\\n        \\n        w = np.zeros((1, n_classes))\\n        g_avg = np.zeros((1, n_classes))\\n        beta_t = 1.0  # Example sequence, adjust as needed\\n        \\n        for t in range(1, max_iter + 1):\\n            for i in indices:\\n                # Compute subgradient of the loss function for sample i\\n                sub_scores = old_scores[i] + w\\n                probs = np.exp(sub_scores) / np.sum(np.exp(sub_scores))\\n                subgradient = -np.eye(n_classes)[y[i]] + probs\\n                \\n                # Update average subgradient\\n                g_avg = (t - 1) / t * g_avg + 1 / t * subgradient\\n                \\n                # Update w using RDA formula\\n                w = w - beta_t / t * g_avg - self.lambda_reg * w\\n        \\n        return w'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def _find_w(self, indices, old_scores, y, max_iter=10):\n",
    "        #n_samples = len(indices)\n",
    "        n_classes = old_scores.shape[1]\n",
    "        \n",
    "        w = np.zeros((1, n_classes))\n",
    "        g_avg = np.zeros((1, n_classes))\n",
    "        beta_t = 1.0  # Example sequence, adjust as needed\n",
    "        \n",
    "        for t in range(1, max_iter + 1):\n",
    "            for i in indices:\n",
    "                # Compute subgradient of the loss function for sample i\n",
    "                sub_scores = old_scores[i] + w\n",
    "                probs = np.exp(sub_scores) / np.sum(np.exp(sub_scores))\n",
    "                subgradient = -np.eye(n_classes)[y[i]] + probs\n",
    "                \n",
    "                # Update average subgradient\n",
    "                g_avg = (t - 1) / t * g_avg + 1 / t * subgradient\n",
    "                \n",
    "                # Update w using RDA formula\n",
    "                w = w - beta_t / t * g_avg - self.lambda_reg * w\n",
    "        \n",
    "        return w'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#   5. Example: Evaluate all methods with 5-fold cross validation\n",
    "###############################################################################\n",
    "def evaluate_all_methods(\n",
    "    dataset_name, \n",
    "    X_new,\n",
    "    X_old,\n",
    "    y_new,\n",
    "    y_old,\n",
    "    localcorrectiontree_cls,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits X,y into 5 folds. In each fold:\n",
    "      1) Create a “train” (fold_train_idx) and “test” (fold_test_idx).\n",
    "      2) Train or fine-tune models on the train set (with any hyperparameter\n",
    "         tuning using that train set only).\n",
    "      3) Evaluate on test.\n",
    "    Accumulate test metrics for each method across folds. \n",
    "    Finally, produce boxplots for accuracy, precision, recall, F1. \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Training old model on X_old, y_old ===\")\n",
    "    old_model = train_old_model(dataset_name, X_old, y_old, model_type='LGBM')\n",
    "    old_models_dict = {\"LGBM\": old_model}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # We’ll store results as: results[model_name]['accuracy'] = [fold1, fold2, ...]\n",
    "    results = {}\n",
    "    all_models = {}\n",
    "    \n",
    "    # (a) old models\n",
    "    for old_m_name, old_m in old_models_dict.items():\n",
    "        all_models[f\"Old_{old_m_name}\"] = old_m\n",
    "        \n",
    "    fold_idx = 1\n",
    "    for train_index, test_index in skf.split(X_new, y_new):\n",
    "        print(f\"\\n========= Fold {fold_idx} =========\")\n",
    "        fold_idx += 1\n",
    "        \n",
    "        X_train, X_test = X_new[train_index], X_new[test_index]\n",
    "        y_train, y_test = y_new[train_index], y_new[test_index]\n",
    "        \n",
    "        # 2) Train the naive/new-only baselines\n",
    "        base_models = train_base_competitor_models(dataset_name, X_train, y_train)\n",
    "        \n",
    "        # 3) Train standard correction methods\n",
    "        competitor_corrected = train_competitor_standard_corrected(\n",
    "            dataset_name, \n",
    "            X_train, \n",
    "            y_train,\n",
    "            old_model=old_model,\n",
    "            base_competitor_models=base_models\n",
    "        )\n",
    "        \n",
    "        # 4) Train LCT to correct each old model\n",
    "        lct_corrected = train_lct(\n",
    "            dataset_name, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            old_models_dict, \n",
    "            localcorrectiontree_cls\n",
    "        )\n",
    "        \n",
    "        # 5) Evaluate each method on the test set. \n",
    "        #    This includes:\n",
    "        #      - The old model(s)\n",
    "        #      - The naive baselines (L1-LR, L2-LR, DT, RF, LGBM)\n",
    "        #      - The standard corrections (L1-LR+, ..., LGBM+C, etc.)\n",
    "        #      - Our LCT corrections\n",
    "        \n",
    "        # (b) naive/new-only\n",
    "        for m_name, m in base_models.items():\n",
    "            all_models[m_name] = m\n",
    "        \n",
    "        # (c) standard corrections\n",
    "        for m_name, m in competitor_corrected.items():\n",
    "            all_models[m_name] = m\n",
    "        \n",
    "        # (d) LCT\n",
    "        #for m_name, m in lct_corrected.items():\n",
    "        #    all_models[m_name] = m\n",
    "        \n",
    "        # Now actually do test predictions\n",
    "        for model_key, model_obj in all_models.items():\n",
    "            \n",
    "            # If model_obj is an LCT, we do old_scores + correction, then argmax\n",
    "            if isinstance(model_obj, LocalCorrectionTree):\n",
    "                \n",
    "                try:\n",
    "                    old_probs = old_models_dict[model_key].predict_proba(X_test)\n",
    "                except:\n",
    "                    preds_ = old_models_dict[model_key].predict(X_test)\n",
    "                    unique_labels = np.unique(preds_)\n",
    "                    n_cls = len(unique_labels)\n",
    "                    old_probs = np.zeros((len(X_test), n_cls))\n",
    "                    for i, lab in enumerate(unique_labels):\n",
    "                        old_probs[preds_ == lab, i] = 1.0\n",
    "                        \n",
    "                corrections_ = model_obj.predict(X_test)  # shape matching old_probs\n",
    "                corrected_scores = old_probs + corrections_\n",
    "                y_pred = np.argmax(corrected_scores, axis=1)\n",
    "                \n",
    "            else:\n",
    "                # Normal scikit-learn model\n",
    "                y_pred = model_obj.predict(X_test)\n",
    "            \n",
    "            # Evaluate metrics\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            # For binary or multi-class, set average appropriately\n",
    "            if dataset_name in ['CTG', '7-point']:\n",
    "                # multi-class => macro avg\n",
    "                prec = precision_score(y_test, y_pred, average='macro')\n",
    "                rec = recall_score(y_test, y_pred, average='macro')\n",
    "                f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            else:\n",
    "                # binary => standard\n",
    "                prec = precision_score(y_test, y_pred)\n",
    "                rec = recall_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            # Store\n",
    "            if model_key not in results:\n",
    "                results[model_key] = {\n",
    "                    'accuracy': [],\n",
    "                    'precision': [],\n",
    "                    'recall': [],\n",
    "                    'f1': []\n",
    "                }\n",
    "            results[model_key]['accuracy'].append(acc)\n",
    "            results[model_key]['precision'].append(prec)\n",
    "            results[model_key]['recall'].append(rec)\n",
    "            results[model_key]['f1'].append(f1)\n",
    "    \n",
    "    # Once done with all folds, produce boxplots for each metric\n",
    "    _plot_results_boxplots(results, dataset_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def _plot_results_boxplots(results_dict, dataset_name):\n",
    "    \"\"\"\n",
    "    Create side-by-side horizontal bar plots (Accuracy, Precision, Recall, F1).\n",
    "    Each bar is the AVERAGE score across all folds for the given model.\n",
    "    The bars are arranged so that base models appear next to their '+'\n",
    "    counterparts, e.g. 'L1-LR' next to 'L1-LR+'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    rows = []\n",
    "    for model_name, metrics in results_dict.items():\n",
    "        n_folds = len(metrics['accuracy'])\n",
    "        for i in range(n_folds):\n",
    "            rows.append({\n",
    "                'models':     model_name,\n",
    "                'accuracy':  metrics['accuracy'][i],\n",
    "                'precision': metrics['precision'][i],\n",
    "                'recall':    metrics['recall'][i],\n",
    "                'f1':        metrics['f1'][i]\n",
    "            })\n",
    "            \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # 2) Compute the AVERAGE across folds for each model\n",
    "    df_mean = df.groupby('models', as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    desired_order = [\n",
    "        \"L1-LR\", \"L1-LR+\",\n",
    "        \"L2-LR\", \"L2-LR+\",\n",
    "        \"DT\", \"DT+\",\n",
    "        \"RF\", \"RF+\",\n",
    "        \"LGBM\", \"LGBM+\", \"LGBM+C\",\n",
    "        \"Ours\", \"Old_LGBM\"\n",
    "    ]\n",
    "    \n",
    "    # Keep only those that actually appear\n",
    "    model_order = [m for m in desired_order if m in df_mean['models'].unique()]\n",
    "\n",
    "    # Set up a whitegrid style and a smaller figure\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\", font_scale=0.8)\n",
    "    \n",
    "    # Adjust the figure size to make it smaller\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(8, 4), sharey=True)\n",
    "\n",
    "    # Helper to plot one metric as a bar chart\n",
    "    def plot_metric(ax, metric_col, title_str):\n",
    "        # Subset & reorder so the bars appear in the desired order\n",
    "        plot_data = df_mean.set_index('models').loc[model_order].reset_index()\n",
    "        \n",
    "        sns.barplot(\n",
    "            data=plot_data,\n",
    "            x=metric_col,       # numeric metric on the x-axis\n",
    "            y='models',          # model on the y-axis\n",
    "            ax=ax, \n",
    "            orient='h',         # horizontal bars\n",
    "            palette='Set2',     # different color per model\n",
    "        )\n",
    "        ax.set_title(f\"{title_str} ({dataset_name})\")\n",
    "        ax.set_xlim([0.0, 1.0])  # Adjust as needed for your data range\n",
    "        ax.legend()\n",
    "        \n",
    "    # Draw each subplot\n",
    "    plot_metric(axes[0], 'accuracy',  'Accuracy')\n",
    "    plot_metric(axes[1], 'precision', 'Precision')\n",
    "    plot_metric(axes[2], 'recall',    'Recall')\n",
    "    plot_metric(axes[3], 'f1',        'F1')\n",
    "    \n",
    "    #plt.title(f\"Comparison plot for_{dataset_name}\")\n",
    "    plt.grid(True, axis='x', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Comparison plot for_{dataset_name}.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#   USAGE EXAMPLE (Pseudocode)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     X_old, y_old, X_new, y_new, feature_names, x \u001b[38;5;241m=\u001b[39m \u001b[43mload_prep_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      8\u001b[0m     X_old, y_old \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_old), np\u001b[38;5;241m.\u001b[39marray(y_old)\n\u001b[1;32m      9\u001b[0m     X_new, y_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_new), np\u001b[38;5;241m.\u001b[39marray(y_new)\n",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m, in \u001b[0;36mload_prep_data\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBank\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m Dataset(dataset_name)\n\u001b[0;32m----> 5\u001b[0m     X, y, feature_names, x \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     X_old, y_old, X_new, y_new \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msplit_old_new_data(X, y)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_old, y_old, X_new, y_new, feature_names, x\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/Datasets/data.py:30\u001b[0m, in \u001b[0;36mDataset.load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdult\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m         adult \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_ucirepo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([adult\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfeatures, adult\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtargets], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m#data = data.dropna()\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/ucimlrepo/fetch.py:97\u001b[0m, in \u001b[0;36mfetch_ucirepo\u001b[0;34m(name, id)\u001b[0m\n\u001b[1;32m     95\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError reading data csv file for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dataset (id=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, \u001b[38;5;28mid\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/Desktop/Projects/Model Correction/model_correction/lib/python3.11/site-packages/pandas/io/common.py:389\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[1;32m    388\u001b[0m             compression \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 389\u001b[0m         reader \u001b[38;5;241m=\u001b[39m BytesIO(\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[1;32m    391\u001b[0m         filepath_or_buffer\u001b[38;5;241m=\u001b[39mreader,\n\u001b[1;32m    392\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m         mode\u001b[38;5;241m=\u001b[39mfsspec_mode,\n\u001b[1;32m    396\u001b[0m     )\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/http/client.py:460\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/http/client.py:583\u001b[0m, in \u001b[0;36mHTTPResponse._read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 583\u001b[0m         chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chunk_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/http/client.py:566\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_next_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/http/client.py:526\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#   USAGE EXAMPLE (Pseudocode)\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    X_old, y_old, X_new, y_new, feature_names, x = load_prep_data('Adult')    \n",
    "\n",
    "    X_old, y_old = np.array(X_old), np.array(y_old)\n",
    "    X_new, y_new = np.array(X_new), np.array(y_new)\n",
    "    \n",
    "    results = evaluate_all_methods(\n",
    "        dataset_name='Adult',\n",
    "        X_new=X_new[:100], \n",
    "        X_old=X_old[:100],\n",
    "        y_new=y_new[:100],\n",
    "        y_old=y_old[:100],\n",
    "        localcorrectiontree_cls=LocalCorrectionTree\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_correction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
